{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for bigainlco/LooGLE contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigainlco/LooGLE\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from rag_components import *\n",
    "from aws_mixtral import mixtral\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "from rich.progress import track\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Specific Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks, summarizer_llm=Settings.llm, text_title=None):\n",
    "    \"\"\"\n",
    "    Generate a summary for each text chunk using the summarizer_llm.\n",
    "\n",
    "    Args:\n",
    "        chunks (list[str]): A list of text chunks to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Summaries of the text chunks.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    if text_title is None:\n",
    "        text_title = \"chunks\"\n",
    "    for chunk in tqdm(chunks, desc=f'Summarizing \"{text_title}\"', leave=False):\n",
    "        if len(summaries) == 0:\n",
    "            prompt = f\"\"\"Summarize the following text conceptually. \\\n",
    "                The summary should paraphrase the original text, be significantly \\\n",
    "                shorter, retain all propositions, and be able to replace the \\\n",
    "                original text. Here is the text to be summarized:\\n\\n{chunk}\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Summarize the following text conceptually in the context of the text that precedes it. \\\n",
    "                The summary should paraphrase the original text, be significantly \\\n",
    "                shorter, retain all propositions, and be able to replace the \\\n",
    "                original text. Here is the the context:\\n\\n{summaries[-1]}\\n\\n\\\n",
    "                Summarize the following text:\\n\\n{chunk}\"\"\"\n",
    "\n",
    "        prompt = re.sub(r\"\\s+\", \" \", prompt)\n",
    "        response = summarizer_llm.complete(prompt).text.strip()\n",
    "        summaries.append(response)\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def summarize_index(text_title, overwrite_existing=False, summarizer_llm=Settings.llm):\n",
    "    \"\"\"\n",
    "    Generate a summary for each text chunk in the index using GPT-4.\n",
    "\n",
    "    Args:\n",
    "        index (VectorStoreIndex): The index containing text chunks to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: An index containing summaries for each text chunk.\n",
    "    \"\"\"\n",
    "    if not overwrite_existing:\n",
    "        existing_index = get_index_by_title(f\"{text_title}_summaries\")\n",
    "        if existing_index:\n",
    "            return existing_index\n",
    "    index = get_index_by_title(text_title)\n",
    "    text_chunk_ids = get_ids_from_index(index)\n",
    "    id_nums = [int(id_.split(\"_\")[-1]) for id_ in text_chunk_ids]\n",
    "    summary_ids = [f\"summary_{id_num}\" for id_num in id_nums]\n",
    "    chunks = [get_text_by_id(index, node_id) for node_id in text_chunk_ids]\n",
    "    summaries = summarize_chunks(chunks, summarizer_llm, text_title=text_title)\n",
    "\n",
    "    summary_index = create_index_from_chunks_with_ids(\n",
    "        summaries, summary_ids, f\"{text_title}_summaries\", overwrite_existing=overwrite_existing\n",
    "    )\n",
    "    return summary_index\n",
    "\n",
    "\n",
    "def answer_reading_comprehension_with_summarization(\n",
    "    question, context_title, context_text, top_k=2, chunk_size=1024, chunk_overlap=200, qa_llm=qa_llm\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question given a context.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_title (str): The title of the context\n",
    "        context_text (str): The text of the context\n",
    "        top_k (int): The number of top matching chunks (or equivalent tokens) to retrieve\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "    text_index = create_index_from_text_with_ids(\n",
    "        context_text, context_title, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    summary_index = summarize_index(context_title, summarizer_llm=mixtral)\n",
    "\n",
    "    combined_index = index_union([summary_index])\n",
    "\n",
    "    # Query the vector store to find top matching chunks\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=combined_index,\n",
    "        similarity_top_k=20,\n",
    "    )\n",
    "    top_chunks = retriever.retrieve(question)\n",
    "\n",
    "    max_tokens = top_k * chunk_size\n",
    "\n",
    "    top_chunks_text = []\n",
    "    top_chunks_text_combined = \"\"\n",
    "    chunks_used = 0\n",
    "    for i, chunk in enumerate(top_chunks):\n",
    "        chunk_text = chunk.node.text\n",
    "        if count_tokens(top_chunks_text_combined) + count_tokens(chunk_text) > max_tokens:\n",
    "            break\n",
    "        else:\n",
    "            top_chunks_text.append(chunk.node.text)\n",
    "            top_chunks_text_combined += chunk.node.text + \" \"\n",
    "            chunks_used += 1\n",
    "    top_chunks = top_chunks[:chunks_used]\n",
    "\n",
    "    # Extract and combine the text from the top matching chunks for use as context\n",
    "    top_chunks_text = [chunk.node.text for chunk in top_chunks]\n",
    "    top_chunks_text_combined = \" \".join(top_chunks_text)\n",
    "\n",
    "    raw_text_chunk_count = 0\n",
    "    summary_chunk_count = 0\n",
    "    for chunk in top_chunks:\n",
    "        if chunk.node.id_.startswith(\"text_chunk_\"):\n",
    "            raw_text_chunk_count += 1\n",
    "        elif chunk.node.id_.startswith(\"summary_\"):\n",
    "            summary_chunk_count += 1\n",
    "\n",
    "    # Use the combined context of top chunks to generate an answer to the question\n",
    "    # Construct a prompt that guides the LLM to consider the context and answer the question subjectively or conceptually\n",
    "    prompt = f\"\"\"Consider the following context with depth and thoughtfulness: {top_chunks_text_combined}\\n\\n\\\n",
    "        Respond to the following question with insight and nuance. Answer concisely, often in one \\\n",
    "        sentence or less and sometimes in the form of a list or structured text. If the question \\\n",
    "        asks you to order events, refer to the events by their number (e.g. \"1. third event, 2. second \\\n",
    "        event, 3. first event\" -> \"3, 2, 1\"). Answer multiple choice questions using the number which \\\n",
    "        corresponds to the correct answer (e.g. \"1. A, 2. B, 3. C\" -> \"2\"). Do not include the \\\n",
    "        question in your answer. \\\n",
    "        \\n\\n\\\n",
    "        Question: {question}\\n\\n\\\n",
    "        Answer: \"\"\"\n",
    "    response = qa_llm.complete(prompt).text\n",
    "    additional_info = {\n",
    "        \"raw_text_chunk_count\": raw_text_chunk_count,\n",
    "        \"summary_chunk_count\": summary_chunk_count,\n",
    "        \"top_chunks_text\": top_chunks_text,\n",
    "    }\n",
    "    return response, additional_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be3dbe21b47455cad2b3572802e75a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Answering questions:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "debug_lim = 100\n",
    "output_file = \"output/mixtral_summary_only_100.jsonl\"\n",
    "test_longdep_qa(answer_reading_comprehension_with_summarization, output_file=output_file, debug_lim=debug_lim, qa_llm=mixtral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge Metrics: {'rouge1': 0.20306477452481553, 'rouge2': 0.05884763085108288, 'rougeL': 0.16059295343233992, 'rougeLsum': 0.17164550855100139}\n",
      "LLM Self-Score: 0.32\n"
     ]
    }
   ],
   "source": [
    "rouge_metrics = get_rouge_metrics(output_file)\n",
    "print(\"Rouge Metrics:\", rouge_metrics)\n",
    "\n",
    "self_score = llm_self_score(output_file, llm=qa_llm)\n",
    "print(\"LLM Self-Score:\", self_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_markdown(d, level=2):\n",
    "    markdown = \"\"\n",
    "    header = \"#\" * level\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            markdown += f\"{header} {key}\\n\\n{dict_to_markdown(value, level+1)}\"\n",
    "        elif isinstance(value, list):\n",
    "            markdown += f\"{header} {key}\\n\\n\"\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    markdown += f\"{dict_to_markdown(item, level+1)}\"\n",
    "                else:\n",
    "                    markdown += f\"- {item}\\n\"\n",
    "            markdown += \"\\n\"\n",
    "        else:\n",
    "            markdown += f\"{header} {key}\\n\\n{value}\\n\\n\"\n",
    "\n",
    "    return markdown\n",
    "\n",
    "\n",
    "question_objects = []\n",
    "with open(output_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        question_objects.append(json.loads(line))\n",
    "\n",
    "markdowns = [dict_to_markdown(obj) for obj in question_objects]\n",
    "\n",
    "with open(f\"output_md/{output_file.split('/')[-1].split('.')[0]}.md\", \"w\") as f:\n",
    "    f.write(\"\\n\\n\".join(markdowns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LongRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

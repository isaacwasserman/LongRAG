{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/baseline3_100.jsonl\") as f:\n",
    "    lines = f.readlines()\n",
    "    baseline_outputs = [json.loads(line) for line in lines]\n",
    "\n",
    "with open(\"output/summarization_with_corresponding_in_context.jsonl\") as f:\n",
    "    lines = f.readlines()\n",
    "    summarization_outputs = [json.loads(line) for line in lines]\n",
    "\n",
    "with open(\"output/question_generation_with_corresponding_in_context.jsonl\") as f:\n",
    "    lines = f.readlines()\n",
    "    question_generation_outputs = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correct_mat = np.array(\n",
    "    [\n",
    "        [output[\"correct\"] for output in baseline_outputs],\n",
    "        [output[\"correct\"] for output in summarization_outputs],\n",
    "        [output[\"correct\"] for output in question_generation_outputs],\n",
    "        [summarization_outputs[i][\"correct\"] or question_generation_outputs[i][\"correct\"] for i in range(100)],\n",
    "    ]\n",
    ").T\n",
    "\n",
    "correct_mat_chunks = np.array_split(correct_mat, 4)\n",
    "# Create side by side imshow for each chunk\n",
    "fig, axs = plt.subplots(1, 4, figsize=(10, 10))\n",
    "for i, chunk in enumerate(correct_mat_chunks):\n",
    "    axs[i].imshow(chunk, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_baseline_indices = [i for i, output in enumerate(baseline_outputs) if output[\"correct\"] == False]\n",
    "correct_summarization_indices = [i for i, output in enumerate(summarization_outputs) if output[\"correct\"] == True]\n",
    "correct_question_generation_indices = [i for i, output in enumerate(question_generation_outputs) if output[\"correct\"] == True]\n",
    "\n",
    "summarization_advantage_indices = set(correct_summarization_indices) & set(incorrect_baseline_indices)\n",
    "question_generation_advantage_indices = set(correct_question_generation_indices) & set(incorrect_baseline_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 5, 13, 22, 32, 33, 48, 51, 58, 59, 62, 68, 79, 93}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_generation_advantage_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"All of historians speak highly of Picardo's work, is this true? Why?\",\n",
       " 'ground_truth': 'False, because some people believe that Parrado destroyed the part of historical and architectural.',\n",
       " 'generated_answer': 'No, because his approach to restoration, prioritizing aesthetics and tourism over historical accuracy and archaeological research, has been a matter of significant controversy and regret among historians and archaeologists.',\n",
       " 'additional_info': {'top_chunks_info': [{'summary_score': 0.8918934984771567,\n",
       "    'generated_question_rank': 0,\n",
       "    'text_score': 0.8131839761491482,\n",
       "    'text_rank': 5,\n",
       "    'generated_question': \"How do contemporary archaeologists and historians view Picardo's work, and what specific issues are they concerned about?\",\n",
       "    'text': 'In 1975 he also developed improvements for one of the earliest existing Paradores, opened in 1929, the castle at Ciudad Rodrigo in the Province of Salamanca in Castilla y León. Controversial legacy of Picardo\\'s Paradores. It was not unusual in Spain in the 1960s and 1970s for the rehabilitation of castles and convents (not all destined to be Paradores) to be carried out without archeological research either before work began, which would have added to expense and delayed the project, or while work was being carried out. Instead, Picardo\\'s rebuilding projects were planned mostly on the basis of his own historical and architectural research. The hotel conversions and the demolition of large parts of monumental buildings without detailed investigation and record-keeping was somewhat frowned upon in the 1960s and 1970s, and over half a century later is seen by archeologists and historians as a matter of significant controversy and regret. Picardo\\'s work at Sigüenza, in particular, converting a castle-palace into a Parador, has been decried as \"medieval scenery for tourist accommodation\".The leading researcher into the architectural history of the Paradores network and its restoration of architectural heritage, Dr María José Rodríguez Pérez, has extensively documented and studied the work of Picardo and his fellow Paradores architects of the 1960s and 1970s in her lengthy and detailed doctoral thesis and subsequent books and publications. She has described the architects\\' objective as being escenografía convincente (convincing set design) to evoke the historical era considered to be of interest to tourists, generally the medieval period. In writing of the new extensions which were designed to be identical to the monuments to which they were attached — Picardo\\'s Parador at Jäen is a good example — she has described them as being \"falso histórico\" (false history) ... \"a replica whose documentary value has been masked or even lost\".In Picardo\\'s defence, his early mentor Fernando Choeca Goitia defined him as \"un arquitecto sue entiende la arquitectura como arte\" (an architect who understands architecture as art). Picardo himself maintained: \"El Arte es eterno ...\"(Art is eternal ...), \"...it is always current. The reconstructions of the castles are really false. If they are Art, they are justified and if they are not, they are truly condemnable.\" Picardo had no qualms about his film set concept of restoration, using modern construction techniques and concealing them with traditional materials, as long as the buildings looked old rather than modern. One Spanish academic, an assistant professor of architecture and design, writing of Picardo\\'s artistry, has stated: \"The end ... justified the means, in such a way that in his work we can find an impressive rib vault supported by a hidden metallic substructure, a coffered ceiling suspended from a concrete slab or a stone retaining wall with a reinforced concrete core.\" He goes on to say that faced with the dilemma of adopting a \"mimetic and conservative attitude or a more modern and disruptive approach\", Picardo claimed supremacy for Art. \"En Arte todo es posible\" (In Art everything is possible), wrote Picardo in 1994. \"A good architect will know how to weigh up both solutions and his sensitivity shall dictate his choice.\"Despite the current views of historians, Picardo\\'s Paradores — particularly those at Jäen, Carmona and Sigüenza — though pastiche, remain amongst the most popular of the network\\'s hotels. One United States travel writer enthused about Jäen: \"I love this parador, so dramatic in its setting, so theatrically conceived ... Inside, the deception is masterly, creating an ambience as old and austere as it is surrealistic and extravagant.\" Other historical restorations. Demonstrating his educated and precise knowledge of classical styles, during his career Picardo carried out restoration works on the Catedral de Cádiz, deleteriously affected by salt from being near the sea, the Real Monasterio de Santa María de Guadalupe, the Catedral de Santa María de Sigüenza, damaged during the Civil War, and in the tiny Ermita del Humilladero in the Sierra de Villuercas. He rehabilitated the Antiguo Palacio del Marqués de Montana (also known as Palacio Domecq) in Jerez, rebuilt the Palacio de Gamazo in Madrid which had been partially demolished three years before, restored the Castillo de San Felipe in Puerto de la Cruz de Tenerife and in his last project worked on the Archivo Histórico Provincial de Salamanca in the old centre of the city in 1995. Fundación Juan March.'},\n",
       "   {'summary_score': 0.8779765758224926,\n",
       "    'generated_question_rank': 1,\n",
       "    'text_score': 0.81313510811807,\n",
       "    'text_rank': 5,\n",
       "    'generated_question': \"Why were Picardo's rebuilding projects for Paradores planned mostly on the basis of his own historical and architectural research?\",\n",
       "    'text': 'In 1975 he also developed improvements for one of the earliest existing Paradores, opened in 1929, the castle at Ciudad Rodrigo in the Province of Salamanca in Castilla y León. Controversial legacy of Picardo\\'s Paradores. It was not unusual in Spain in the 1960s and 1970s for the rehabilitation of castles and convents (not all destined to be Paradores) to be carried out without archeological research either before work began, which would have added to expense and delayed the project, or while work was being carried out. Instead, Picardo\\'s rebuilding projects were planned mostly on the basis of his own historical and architectural research. The hotel conversions and the demolition of large parts of monumental buildings without detailed investigation and record-keeping was somewhat frowned upon in the 1960s and 1970s, and over half a century later is seen by archeologists and historians as a matter of significant controversy and regret. Picardo\\'s work at Sigüenza, in particular, converting a castle-palace into a Parador, has been decried as \"medieval scenery for tourist accommodation\".The leading researcher into the architectural history of the Paradores network and its restoration of architectural heritage, Dr María José Rodríguez Pérez, has extensively documented and studied the work of Picardo and his fellow Paradores architects of the 1960s and 1970s in her lengthy and detailed doctoral thesis and subsequent books and publications. She has described the architects\\' objective as being escenografía convincente (convincing set design) to evoke the historical era considered to be of interest to tourists, generally the medieval period. In writing of the new extensions which were designed to be identical to the monuments to which they were attached — Picardo\\'s Parador at Jäen is a good example — she has described them as being \"falso histórico\" (false history) ... \"a replica whose documentary value has been masked or even lost\".In Picardo\\'s defence, his early mentor Fernando Choeca Goitia defined him as \"un arquitecto sue entiende la arquitectura como arte\" (an architect who understands architecture as art). Picardo himself maintained: \"El Arte es eterno ...\"(Art is eternal ...), \"...it is always current. The reconstructions of the castles are really false. If they are Art, they are justified and if they are not, they are truly condemnable.\" Picardo had no qualms about his film set concept of restoration, using modern construction techniques and concealing them with traditional materials, as long as the buildings looked old rather than modern. One Spanish academic, an assistant professor of architecture and design, writing of Picardo\\'s artistry, has stated: \"The end ... justified the means, in such a way that in his work we can find an impressive rib vault supported by a hidden metallic substructure, a coffered ceiling suspended from a concrete slab or a stone retaining wall with a reinforced concrete core.\" He goes on to say that faced with the dilemma of adopting a \"mimetic and conservative attitude or a more modern and disruptive approach\", Picardo claimed supremacy for Art. \"En Arte todo es posible\" (In Art everything is possible), wrote Picardo in 1994. \"A good architect will know how to weigh up both solutions and his sensitivity shall dictate his choice.\"Despite the current views of historians, Picardo\\'s Paradores — particularly those at Jäen, Carmona and Sigüenza — though pastiche, remain amongst the most popular of the network\\'s hotels. One United States travel writer enthused about Jäen: \"I love this parador, so dramatic in its setting, so theatrically conceived ... Inside, the deception is masterly, creating an ambience as old and austere as it is surrealistic and extravagant.\" Other historical restorations. Demonstrating his educated and precise knowledge of classical styles, during his career Picardo carried out restoration works on the Catedral de Cádiz, deleteriously affected by salt from being near the sea, the Real Monasterio de Santa María de Guadalupe, the Catedral de Santa María de Sigüenza, damaged during the Civil War, and in the tiny Ermita del Humilladero in the Sierra de Villuercas. He rehabilitated the Antiguo Palacio del Marqués de Montana (also known as Palacio Domecq) in Jerez, rebuilt the Palacio de Gamazo in Madrid which had been partially demolished three years before, restored the Castillo de San Felipe in Puerto de la Cruz de Tenerife and in his last project worked on the Archivo Histórico Provincial de Salamanca in the old centre of the city in 1995. Fundación Juan March.'}]},\n",
       " 'correct': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_generation_outputs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"All of historians speak highly of Picardo's work, is this true? Why?\",\n",
       " 'ground_truth': 'False, because some people believe that Parrado destroyed the part of historical and architectural.',\n",
       " 'generated_answer': 'Yes, because his architectural drawings were described as \"magnificent\" by Luis Menéndez-Pidal y Álvarez, his work for Paradores de Turismo is highly regarded by professionals and hotel guests alike, and his contributions to historical preservation and restoration were aligned with the influential \"Manifiesto de la Alhambra,\" earning respect from peers and historians.',\n",
       " 'additional_info': {'raw_text_chunk_count': 2,\n",
       "  'summary_chunk_count': 0,\n",
       "  'top_chunks_text': ['Picardo was regarded as an \"outstanding\" muralist.At the same time his drawings of buildings and architectural details were published as illustrations in a best-selling textbook on monumental and historic Spanish architecture, Arquitectura Popular Española, by the restoration and conservation architect Leopoldo Torres Balbás. Picardo travelled around Spain with him, making a multitude of detailed drawings of vernacular architectural elements for Balbás\\' books.. Picardo\\'s published architectural drawings were highly regarded. They were described as \"magnificent\" by the leading Spanish restoration architect Luis Menéndez-Pidal y Álvarez.In 1959 Picardo was given an unusual commission: to design a pack of baraja de naipes (playing cards) for exclusive use as advertising material by the Spanish fashion brand Loewe. With much imagination he personalised the characters he portrayed, for instance rendering the King of Hearts as the Emperor Charlemagne, the King of Clubs as Goliath, the King of Diamonds as Julius Caesar and the King of Clubs as Alexander the Great. They were produced in colour by the Spanish firm Naipes Heraclio Fournier and surviving packs are much in demand by collectors. Another games design produced by Picardo at much the same time was a set of wooden chess pieces formed in tall, slender, conical shapes and, with the exception of the pawns, surmounted by intricate and delicate indications of the pieces\\' types. It is dated to 1960.Around 1960 Picardo was rewarded by the Dirección General de Arquitectura (DGA) for the many illustrations he had provided for the DGA\\'s Boletín since he was a student with the publication of a small book, Dibujos de José Luis Picardo (Drawings of José Luis Picardo). More than 60 drawings appear in the book, both illustrations and humorous cartoons, and the foreword compares Picardo\\'s work to illustrators such as the Romanian-American Saul Steinberg and in Britain Osbert Lancaster and Hugh Casson. The book is long out of print and virtually unknown in Spain, and not at all elsewhere, but is available second-hand. Paradores de Turismo. From the early 1960s to 1985 Picardo dedicated much of his professional life to the state-run hotel chain, Paradores de Turismo de España. He had for some time carried out minor work for the Ministerio de Información y Turismo which controlled the hotel network. For the purposes of tourism the Ministry and its forebears had for over 30 years rehabilitated rundown and sometimes ruined historic buildings such as castles and convents and converted them into luxury hotels in a style that went beyond ordinary hotel use. In the early 1960s, as Spanish tourism increased, the Ministry decided to rapidly expand its Parador operation (which would within a decade grow from 40 to 83 establishments) and Picardo, with his previous experience of historical restoration and his abiding interest in historical and vernacular buildings, was seen by the Ministry be a suitable architect to take on much of this type of work.. Picardo began working for Paradores on a series of restorations of old, monumental buildings and sometimes building new establishments adjacent to ruined monuments in a style that faithfully copied their original designs. His hybrid conversions maintained and often embellished the monuments\\' ancient appearance while at the same time finding inspiration in them for the style of luxurious modern hotel arrangements the authorities required.. A wealth of Picardo\\'s drawings for his Paradores projects survive. There are large collections of extensively detailed plans which cover his designs from whole Paradores to the smallest detail of door furniture. There are axonometric before-and-after drawings of the buildings and the landscapes around them. There are bird\\'s eye views exercising his mastery of perspective and his spatial vision. They all show meticulous skill.For nearly twenty years, from the early 1960s to his last work for the Paradores in the 1980s, Picardo carried out eleven major reconstructions of historical buildings and/or erected sympathetic and imitative new constructions abutting them or rising from their ruined foundations. With a number he returned to build additions to his earlier work. He also worked on a number of other Parador projects which for various reasons did not reach fruition. His eleven Parador masterworks encouraged other Spanish architects to work in the same vein, and Portuguese architects, too, in the similar state-run chain of hotels in Portugal, the Pousadas de Portugal. Picardo\\'s work for Paradores de Turismo is highly regarded by other professionals, and also by hotel guests who revel in the historical imagery and romance of his work. Parador de Guadalupe: Zurbarán.',\n",
       "   'Early life. Picardo was born in Jerez de la Frontera, in the Province of Cádiz in Andalucía, Spain on 18 June 1919. His father was Alvaro Picardo de Celis and his mother\\'s family name was Castellón. He had four brothers, one of whom died in infancy. His father died in 1929 when Picardo was ten years old. With his mother and his brothers he moved to Madrid, Spain. He enrolled at the newly created Instituto de Bachillerato Cervantes for his high school education. On completing school he initially wanted to join the navy, but was frustrated by the closure of the military academies in Madrid during the Second Spanish Republic. He turned to the study of law, but was frustrated again, this time by the start of the Spanish Civil War in July 1936 when he was in the middle of his course. He had just celebrated his seventeenth birthday. Training in architecture. To avoid being evacuated from Madrid when the Spanish Civil War began, Picardo joined the studio of the architect Luis Moya Blanco, a professor 15 years his senior at the Escuela Técnica Superior de Arquitectura de Madrid (Higher Technical School of Architecture of Madrid). Impressed by Picardo\\'s abilities, Moya Blanco encouraged Picardo to abandon law and take up a career in architecture.. The Civil War and the dictatorial regime that followed it resulted in fewer architects in Spain. Some of those who had prospered during the Republic did not survive the war. Others had gone into exile or had been professionally disqualified. Under decree by the dictator Francisco Franco the Dirección General de Aquitectura (General Directorate of Architecture) was set up to control architecture in Spain and collaborate in what his regime called la reconstrucción nacional (national reconstruction). Many architects were required to be subordinate to it. Against this background, in 1945 Picardo entered the Escuela Técnica Superior de Arquitectura de Madrid.From the beginning of Picardo\\'s studies, his abilities in painting and drawing — in particular his mastery of perspective — drew him to the attention of a number of architects who praised him highly. While he was still a student, architects commissioned murals from him for the interiors of their buildings, and employed him within their practices for the graphic representations and perspectives of their plans. Picardo executed his first professional mural painting at the age of 20 in 1939 in the Cine Fígaro (Figaro Cinema) in Madrid, commissioned by his architecture mentor Luis Moya Blanco. The painting of murals was the main source of income for Picardo during his youth and early career.As a student Picardo also began to illustrate many articles and later several covers for the Spanish architectural magazines Revista Nacional de Arquitectura and the Boletín de la Dirección General de Arquitectura. His drawings in these publications have been described as showing \"increasing sophistication\" and being of \"complexity and extraordinary quality\". Particularly noted in his post-student days were illustrations portraying Madrid in the 1950s and 1960s, the Spanish protectorate in Morocco, and sketches of the Canarias (Canary Islands) in 1953. He also showed a growing interest in historic architecture, in particular its preservation and restoration. Picardo completed his training by making increasingly numerous travels to study buildings around Spain and abroad. His investigative journeys around the Iberian Peninsula awakened in him an intense interest in its historical and vernacular architecture. He was described as an \"outstanding\" student. Early career. Architect. On qualifying in 1951, Picardo pursued his interest in historical architecture by collaborating on a number of building preservation and restoration projects with the Spanish architect and architectural historian Fernando Chueca Goitia, who was 8 years his senior. Chueca\\'s appeal to Picardo was the older man\\'s lengthy research into what he saw as the unchanging elements of Spanish architecture that maintained their constancy despite political and religious changes. Picardo was one of the 24 signatories of the \\'\\'Manifiesto de la Alhambra\\'\\' of 1952, described as one of “the most remarkable texts in the histiography of 20th-century Spanish architecture\", of which Chueca was the main instigator. The manifesto collected the reflections of a group of architects (Picardo among them) and \"sought inspiration in the design\" of the Alhambra in Granada, Andalucía for \"a distinctively Spanish form of modern architecture\". This inspiration was to guide much of Picardo\\'s work throughout his career.']},\n",
       " 'correct': False}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_outputs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_bools = \"\"\"False\n",
    "True\n",
    "True\n",
    "False\n",
    "True\n",
    "True\n",
    "False\n",
    "False\n",
    "True\n",
    "True\n",
    "False\n",
    "True\n",
    "False\n",
    "False\n",
    "True\n",
    "True\n",
    "True\n",
    "False\n",
    "False\n",
    "True\n",
    "False\n",
    "True\n",
    "False\n",
    "False\n",
    "True\n",
    "False\n",
    "True\n",
    "False\n",
    "False\n",
    "True\n",
    "True\n",
    "False\n",
    "True\n",
    "True\n",
    "False\n",
    "True\n",
    "True\n",
    "True\n",
    "False\n",
    "False\n",
    "False\n",
    "True\n",
    "False\n",
    "False\"\"\"\n",
    "\n",
    "manual_bools = manual_bools.split(\"\\n\")\n",
    "manual_bools = [b == \"True\" for b in manual_bools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 22)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"output/baseline_mistral_large_100_chunksize512.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    baseline_outputs = [json.loads(line) for line in lines]\n",
    "\n",
    "bools = [output[\"correct\"] for output in baseline_outputs]\n",
    "\n",
    "sum(bools[: len(manual_bools)]), sum(manual_bools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.smart_pdf_loader import SmartPDFLoader\n",
    "\n",
    "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\"  # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "pdf_loader = SmartPDFLoader(llmsherpa_api_url=llmsherpa_api_url)\n",
    "documents = pdf_loader.load_data(pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='7200d53c-9f1b-4373-a583-0e1c330aec72', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0e76ea69-c308-48b9-a3f3-eced1689027d', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\\nBART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\\nIt uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\\nWe evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where spans of text are replaced with a single mask token.\\nBART is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks.\\nIt matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\\nBART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\\nWe also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most inﬂuence end-task performance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c9767d5e-47ce-4734-bfa3-6d5d310708d9', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\\nThe most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\\nRecent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\\nHowever, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3cc96c92-04b3-42f1-b806-27b54ef8bdb1', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\\nBART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\\nPretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\\nBART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7afeb1cd-c9c8-46a5-b28a-ed02c95d9b63', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\\nA key advantage of this setup is the noising ﬂexibility; arbitrary transformations can be applied to the original text, including changing its length.\\nWe evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\\nThis approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5f8a2ebc-4d0d-4343-83e8-55ed2538e70a', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\\nBART is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks.\\nIt matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\\nFor example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cf863492-178c-4c55-8a7f-2153db4b371c', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\\nBART also opens up new ways of thinking about ﬁne tuning.\\nWe present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\\nThese layers are trained to essentially translate the foreign language to noised', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0a756dae-0d9c-44e1-ae71-258420db188f', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\\nAutoregressive Decoder', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9550bf46-48f3-46ac-8eda-11ff2d899098', embedding=None, metadata={'chunk_type': 'list_item'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\\nMissing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\\nHowever words can only condition on leftward context, so it cannot learn bidirec- tional interactions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d06ee9d4-83d0-417a-88c5-5a8dfa0109c3', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\\nused for generation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0bf037a5-1abe-4344-8793-567b406df01b', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\\n | Bidirectional Encoder | Autoregressive Decoder\\n | A _ B _ E <s> A B C D\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f43dfcec-6f25-4b40-a45a-e6df5ee5f706', embedding=None, metadata={'chunk_type': 'list_item'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\\nHere, a document has been corrupted by replacing spans of text with mask symbols.\\nThe corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\\nFor ﬁne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the ﬁnal hidden state of the decoder.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='16793f7b-6bac-4717-a1ba-718d6d7c16da', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6837aaa-db93-4846-9e4d-3349cabbf155', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\\nThis approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='87bc1f33-9fb2-496b-88e7-32ab9a5c25a2', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\\nThis study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\\nWe ﬁnd that BART exhibits the most consistently strong performance across the full range of tasks we consider.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='737c5f5d-4dc2-4da1-a912-1f1345dd22e0', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\\nIt is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\\nFor pre-training, we optimize the negative log likelihood of the original document.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='417df0b0-0962-4390-b583-7300d320d43d', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\\nFor our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\\nThe architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the ﬁnal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\\nIn total, BART contains roughly 10% more parameters than the equivalently sized BERT model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a399278a-55ed-4b79-9431-3a33e5366b00', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\\nBART is trained by corrupting documents and then optimizing a reconstruction loss—the cross-entropy between the decoder’s output and the original document.\\nUnlike existing denoising autoencoders, which are tailored to speciﬁc noising schemes, BART allows us to apply any type of document corruption.\\nIn the extreme case, where all information about the source is lost, BART is equivalent to a language model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='86ebcf48-e1a5-47a5-aa9a-54893198155e', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\\nWe experiment with several previously proposed and novel transformations, but we believe there is a signiﬁcant potential for development of other new alternatives.\\nThe transformations we used are summarized below, and examples are shown in Figure 2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2cf37d25-0f44-432d-a29e-940a0aabb096', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='830d0909-26a2-4069-81e7-aeb8ac4ea2c0', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\\nToken Deletion Random tokens are deleted from the input.\\nIn contrast to token masking, the model must decide which positions are missing inputs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db013130-fa1a-408d-b390-4c2dcbed0203', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\\n | A _C . _ E . | D E . A B C . | C . D E . A B\\n | --- | --- | ---\\n |  | Sentence Permutation | Document RotationToken Masking\\n |  | A B C . D E .A . C . E . | A _ . D _ E .\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7efb6ab9-7364-4968-b878-a36953a2a8f5', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text Inﬁlling\\nFigure 2: Transformations for noising the input that we experiment with.\\nThese transformations can be composed.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d27817b3-4754-4080-90b6-10ed7f522842', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text Inﬁlling\\nText Inﬁlling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (λ = 3).\\nEach span is replaced with a single [MASK] token.\\n0-length spans correspond to the insertion of [MASK] tokens.\\nText inﬁlling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\\nText inﬁlling teaches the model to predict how many tokens are missing from a span.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='442a14ec-41a6-427b-b679-2c439e664e42', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text Inﬁlling\\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shufﬂed in a random order.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6e260a6d-11b1-4fbe-9ad6-f17b2b34f77a', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text Inﬁlling\\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\\nThis task trains the model to identify the start of the document.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ac5020a-1f54-49a5-9ee1-cd99e3f24d5d', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\\nThe representations produced by BART can be used in several ways for downstream applications.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='58bbe4cf-319a-4fa6-a2e6-ecfa5bd6a4cf', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classiﬁcation Tasks\\nFor sequence classiﬁcation tasks, the same input is fed into the encoder and decoder, and the ﬁnal hidden state of the ﬁnal decoder token is fed into new multi-class linear classiﬁer.\\nThis approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dcbc5dee-0975-453d-b3fc-f95d9aadc371', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classiﬁcation Tasks\\nFor token classiﬁcation tasks, such as answer endpoint classiﬁcation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\\nThis representation is used to classify the token.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2f5ee88f-5b26-4f59-b71f-fcb4c1d74889', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\\nBecause BART has an autoregressive decoder, it can be directly ﬁne tuned for sequence generation tasks such as abstractive question answering and summarization.\\nIn both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\\nHere, the encoder input is the input sequence, and the decoder generates outputs autoregressively.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0b4d27a4-170a-4f74-9c48-2391ddb5fa3f', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\\nWe also explore using BART to improve machine translation decoders for translating into English.\\nPrevious work Edunov et al.\\n(2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\\nWe show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b2b94aae-4a17-4226-8789-e6fbedf51d97', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\\nMore precisely, we replace BART’s encoder embedding layer with a new randomly initialized encoder.\\nThe model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\\nThe new encoder can use a separate vocabulary from the original BART model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='174a9d2b-6afe-4173-9808-6816ff401cde', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\\nIn the ﬁrst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART’s encoder ﬁrst layer.\\nIn the second step, we train all model parameters for a small number of iterations.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='750a3576-8cbb-4c09-936f-248992d97ae6', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\\nBART supports a much wider range of noising schemes during pre-training than previous work.\\nWe compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in §5.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='75713ed3-267f-4bdc-852f-8cde87b9ce47', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\\nWhile many pre-training objectives have been proposed, fair comparisons between these have been difﬁcult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and ﬁne-tuning procedures.\\nWe', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='45dfb0d4-3fc6-4983-82ce-08612a9000bb', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\\nA\\nB\\nC\\nD E', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='63218d16-a329-4f14-9ee8-4e537b65651c', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\\n | Pre-trained Encoder | Pre-trained Decoder\\n | --- | ---\\n | Pre-trained Encoder | Pre-trained Decoder\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='728f3792-2208-482d-b1e8-b4cf83ee1871', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\\n | <s> A | B | C | D\\n | --- | --- | --- | ---\\n | Randomly Initialized Encoder\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='67d2da23-2488-4fdc-b3a1-d4fb26045423', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nα\\nβ\\nγ\\nδ ε', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fdf3188d-fa38-4fe7-8f7c-26078eefde89', embedding=None, metadata={'chunk_type': 'list_item'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\n(a) To use BART for classiﬁcation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the ﬁnal output is used.\\n(b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\\nThe new encoder can use a disjoint vocabulary.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62d8e6f0-9987-4c50-a3a7-9da5cf5fe2a6', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nFigure 3: Fine tuning BART for classiﬁcation and translation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='472a29dc-3f4a-4be7-885c-08c47ca37baf', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\\nWe aim, as much as possible, to control for differences unrelated to the pre-training objective.\\nHowever, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\\nFor reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\\nWe compare the following approaches:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a5817a51-d88d-4091-ae59-2f338c6c94f4', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\\nThis model is equivalent to the BART decoder, without cross-attention.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d275654-7190-4873-b1c5-59cab6da989c', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\\nFor consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='58774e1b-31db-4b84-a525-b3ac0f97540d', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='61e033bd-fe41-43b1-bde4-0889beb7d347', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\\nSelf attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the ﬁrst 50% of tokens unmasked and a left-to-right mask for the remainder.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='de6e114e-2e36-4f4a-9383-9edb6756975d', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b03c5fcc-0c2a-487b-88fa-32ad0126ed09', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to efﬁciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fe46da35-990a-4d41-b808-8a8dbef96aa9', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as preﬁx to the target in the decoder, with a loss only on the target part of the sequence.\\nWe ﬁnd the former works better for BART models, and the latter for other models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='23ecbe40-08dd-4d48-a437-0fa231a8c546', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\\nTo most directly compare our models on their ability to model their ﬁne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='07cf4ab0-ab62-4c14-8708-a7fdbe879ddf', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\\nAnswers are text spans extracted from a given document context.\\nSimilar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\\nThe model includes classiﬁers to predict the start and end indices of each token.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6dd07a8a-82bb-4a1e-9460-4f453e02a3fd', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\\nMNLI (Williams et al., 2017), a bitext classiﬁcation task to predict whether one sentence entails another.\\nThe ﬁne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\\nIn contrast to BERT, the representation of the EOS token is used to classify the sentences relations.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='47cdaa99-edcf-4941-a925-74d94a7b9e8f', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\\nModels generate answers conditioned on the concatenation of a question and supporting documents.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='94547c95-de7e-443a-96f6-3b657079815a', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60479a8a-5372-4ca3-9875-a28db4a5bb3b', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6c82fc4e-16e0-4d54-884b-658a0f458c28', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\\nSummaries here are typically closely related to source sentences.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4bc10af-26e2-4a0d-8137-4685f55d27e8', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\\nResults are shown in Table 1.\\nSeveral trends are clear:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='61d7098c-8fbb-417b-b3bb-2b7e55f87c7e', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\\n | --- | --- | --- | --- | --- | --- | ---\\n | F1 Acc PPL PPL PPL PPL\\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\\n | w/ Text Inﬁlling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\\n | w/ Sentence Shufﬂing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\\n | w/ Text Inﬁlling + Sentence Shufﬂing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='decd19df-836d-4ca0-85fa-8d4350ac146a', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\\nTable 1: Comparison of pre-training objectives.\\nAll models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\\nEntries in the bottom two blocks are trained on identical data using the same code-base, and ﬁne-tuned with the same procedures.\\nEntries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpliﬁed to focus on evaluation objectives (see §4.1).\\nPerformance varies considerably across tasks, but the BART models with text inﬁlling demonstrate the most consistently strong performance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b85482d-6c1b-4c6f-b9e1-bc640496cd02', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\\nPerformance of pre-training methods varies signiﬁcantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\\nFor example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a6c04ea2-4466-4a6a-96fc-bf16be592920', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\\nThe successful methods either use token deletion or masking, or self-attention masks.\\nDeletion appears to outperform masking on generation tasks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='81f0f784-ec73-4c80-883a-f1156be5739f', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b31cb32f-e9c2-41c7-8eec-63790760def4', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classiﬁcation decisions.\\nHowever, BART achieves similar performance with only half the number of bidirectional layers.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b6b11043-d0b1-472f-ae06-ce9baae981cf', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\\nSome of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b8a54a2f-143b-4dd3-9a02-cb570a1dd989', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\\nA pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbefbe8c-cfd1-4b52-b1c7-e1e10c52cc15', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\\nBART achieves the most consistently strong performance.\\nWith the exception of ELI5, BART models using text-inﬁlling perform well on all tasks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bf35d330-494d-41a0-a28e-e303cc7bd198', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\\nTo test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='65880dc8-d741-46d1-8a50-0661c0df8b32', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\\nFollowing RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\\nDocuments are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\\nBased on the results in Section §4, we use a combination of text inﬁlling and sentence permutation.\\nWe mask 30% of tokens in each document, and permute all sentences.\\nAlthough sentence permutation only shows signiﬁcant additive gains', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5c6805da-a861-4e2f-9d03-aa3b1d00a2a3', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9c0e169d-aa8d-4743-b1cf-61474f12e63d', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\\nTable 2: Results for large models on SQuAD and GLUE tasks.\\nBART performs comparably to RoBERTa and XLNet, suggesting that BART’s uni-directional decoder layers do not reduce performance on discriminative tasks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='de56bc45-20c8-4471-91a9-2ae98ab8f300', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\\n |  | CNN/DailyMail | XSum\\n | --- | --- | ---\\n |  | R1 | R2 | RL | R1 | R2 | RL\\n | --- | --- | --- | --- | --- | --- | ---\\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a8529ea-3d54-4176-b0ce-c3488e6dbaa6', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\\nTable 3: Results on two standard summarization datasets.\\nBART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c71e64f0-cf3a-438c-bc68-726d09fd6365', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\\nTo help the model better ﬁt the data, we disabled dropout for the ﬁnal 10% of training steps.\\nWe use the same pre-training data as Liu et al.\\n(2019), consisting of 160Gb of news, books, stories, and web text.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8344bd53-f596-4eae-874c-bcfde095fc89', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b771a6e9-4d68-4930-a1e8-a2c520b91548', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\\nOverall, BART performs similarly, with only small differences between the models on most tasks.\\nsuggesting that BART’s improvements on generation tasks do not come at the expense of classiﬁcation performance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6a3a11e7-b0b4-4908-990a-96223f3ca48e', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\\nWe also experiment with several text generation tasks.\\nBART is ﬁne-tuned as a standard sequence-to-sequence model from the input to the output text.\\nDuring ﬁnetuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\\nDuring generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='19290157-374f-4533-a0a9-bf4b2dcc0c74', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\\n |  | ConvAI2 | Valid F1 Valid PPL\\n | --- | --- | ---\\n | Seq2Seq + Attention | 16.02 | 35.07\\n | --- | --- | ---\\n | Best System | 19.09 | 17.51\\n | BART | 20.72 | 11.85\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='43e81eac-7753-4cad-ba6b-00df67ae953a', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\\nTable 4: BART outperforms previous work on conversational response generation.\\nPerplexities are renormalized based on ofﬁcial tokenizer for ConvAI2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce70da13-2170-49f2-adad-f0aa36acd98d', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='87779e4b-ae2d-480e-82be-04e49077defa', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\\nSummaries in the CNN/DailyMail tend to resemble source sentences.\\nExtractive models do well here, and even the baseline of the ﬁrst-three source sentences is highly competitive.\\nNevertheless, BART outperforms all existing work.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a589edca-1b0d-42bf-9b56-e5ce5679b722', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\\nBART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics—representing a signiﬁcant advance in performance on this problem.\\nQualitatively, sample quality is high (see §6).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6a7e9403-eb00-48b3-bc93-9bcb308e2579', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speciﬁed persona.\\nBART outperforms previous work on two automated metrics.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7c2eb722-877f-4ae5-9520-affd95c67be4', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\\n | Best Extractive | 23.5 | 3.1 | 17.5\\n | --- | --- | --- | ---\\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='729fc2f9-1e7e-444c-bf31-68b5d390d20e', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\\nComparison models are from Fan et al.\\n(2019).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6dbafcc8-94f7-48a9-941d-3cba67d7fc78', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='82b5b16e-9d0a-43cb-bfc2-b117a9d1f825', embedding=None, metadata={'chunk_type': 'table'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\\n | 7 Related Work | \\n | --- | ---\\n | Baseline | 36.80\\n | Fixed BART 36.29 Tuned BART 37.96\\n | Table 6: The performance (BLEU) of baseline and BART on WMT’16 RO-EN augmented with backtranslation data. BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\\n | Abstractive QA We use the recently proposed ELI5 dataset to test the model’s ability to generate long freeform answers. We ﬁnd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speciﬁed by the question.\\n | 5.4 Translation\\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. (2016). We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in §3.4. Experiment results are presented in Table 6. We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). We show the performance of both steps of our model in the ﬁxed BART and tuned BART rows. For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. We use a beam width of 5 and a length penalty of α = 1. Preliminary results suggested that our approach was less effective without back-translation data, and prone to overﬁtting—future work should explore additional regularization techniques.\\n | 6 Qualitative Analysis\\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. To understand BART’s performance beyond automated metrics, we analyse its generations qualitatively.\\n | Table 7 shows example summaries generated by BART. Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model’s training data. Following Narayan et al. (2018), we remove the ﬁrst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\\n | Unsurprisingly, model output is ﬂuent and grammatical English. However, model output is also highly abstractive, with few phrases copied from the input. The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). In the ﬁrst example, inferring that ﬁsh are protecting reefs from global warming requires non-trivial inference from the text. However, the claim that the work was published in Science is not supported by the source.\\n | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\\n | Early methods for pretraining were based on language models. GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks. ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features. Radford et al. (2019) demonstrated that very large language models can act as unsupervised multitask models.\\n | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words. Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\\n | UniLM (Dong et al., 2019) ﬁne-tunes BERT with an ensemble of masks, some of which allow only leftward context. Like BART, this allows UniLM to be used for both generative and discriminative tasks. A difference is that UniLM predictions are conditionally indepen- dent, whereas BART’s are autoregressive. BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\\n | MASS (Song et al., 2019) is perhaps the most similar model to BART. An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\\n | XL-Net (Yang et al., 2019) extends BERT by pre-\\n | Source Document (abbreviated) | BART Summary\\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when ﬁsh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching. The researchers sug- gested the algae, like warming temperatures, might render the corals’ chemical defenses less effective, and the ﬁsh were pro- tecting the coral by removing the algae. | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\\n | Sacoolas, who has immunity as a diplomat’s wife, was involved in a trafﬁc collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford. He said, “I hope that Anne Sacoolas will come back if we can’t resolve it then of course I will be raising it myself personally with the White House.” | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas’ diplo- matic immunity with the White House.\\n | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday. On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone. Then both na- tions issued statements speaking of an imminent incursion into northeast Syria . On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion. Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey’s incursion into the region. | \\n | This is the ﬁrst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time. It was not, however, an ofﬁcially sanctioned world record, as it was not an”open race” of the IAAF. His time was 1 hour 59 minutes 40.2 seconds. Kipchoge ran in Vienna, Austria. It was an event speciﬁcally designed to help Kipchoge break the two hour barrier. | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\\n | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildﬁres. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. | Power has been turned off to millions of customers in California as part of a power shutoff plan.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='056bb262-ae43-4490-890f-747c8778e3a7', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\\nFor clarity, only relevant excerpts of the source are shown.\\nSummaries combine information from across the article and prior knowledge.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c1636a9a-1a44-4980-9882-01fa2517eea8', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\\ndicting masked tokens auto-regressively in a permuted order.\\nThis objective allows predictions to condition on both left and right context.\\nIn contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1634bef4-dcb4-4258-b8d8-b8f3e32b7e23', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\\nSeveral papers have explored using pre-trained representations to improve machine translation.\\nThe largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\\nOther work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\\nWe show how BART can be used to improve machine translation decoders.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='94bb715d-77a4-4c89-949c-3b3bb9a74c02', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='8 Conclusions\\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\\nBART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\\nFuture work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speciﬁc end tasks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6b72ebe9-2e31-409e-81d4-74fb8b5bb158', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nEneko Agirre, Llu’is M‘arquez, and Richard Wicentowski (eds.).\\nProceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\\nAssociation for Computational Linguistics, Prague, Czech Republic, June 2007.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eaf49c18-98c2-41d8-a040-fcfe568b58bb', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\\nThe PASCAL recognising textual entailment challenge.\\nIn Machine learning challenges.\\nevaluating predictive uncertainty, visual object classiﬁcation, and recognising tectual entailment, pp.\\n177– 190.\\nSpringer, 2006.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5671a03a-0f2a-4a76-afbc-6338272732fd', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\nBERT: Pre-training of deep bidirectional transformers for language understanding.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\\n4171– 4186, Minneapolis, Minnesota, June 2019.\\nAssociation for Computational Linguistics.\\ndoi: 10.18653/ v1/N19-1423.\\nURL https://www.aclweb.\\norg/anthology/N19-1423.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1458edc0-a6ae-4c2e-944f-3d87e813a7cc', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\\nThe second conversational intelligence challenge (convai2).\\narXiv preprint arXiv:1902.00098, 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='319027e8-8b72-48c5-9c4c-df791409bebb', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nWilliam B Dolan and Chris Brockett.\\nAutomatically constructing a corpus of sentential paraphrases.\\nIn Proceedings of the International Workshop on Paraphrasing, 2005.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a8a57c70-2924-403e-ae7f-a54c27fad124', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\\nUniﬁed language model pretraining for natural language understanding and generation.\\narXiv preprint arXiv:1905.03197, 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c1eda74f-4555-42cb-a9b2-1a26296ab6d3', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nSergey Edunov, Alexei Baevski, and Michael Auli.\\nPre-trained language model representations for language generation.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='986972e6-c5da-40ce-9606-bc1737b140e8', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nAngela Fan, David Grangier, and Michael Auli.\\nControllable abstractive summarization.\\narXiv preprint arXiv:1711.05217, 2017.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eea273e5-f4b4-45f4-a691-ceff3f2d7662', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\\nEli5: Long form question answering.\\narXiv preprint arXiv:1907.09190, 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4586603d-158c-446e-b5fc-c68bf9d5b394', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nDan Hendrycks and Kevin Gimpel.\\nGaussian error linear units (gelus).\\narXiv preprint arXiv:1606.08415, 2016.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e8998668-4d4c-4c0e-aca3-ba3ac0821517', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\\nTeaching machines to read and comprehend.\\nIn Advances in neural information processing systems, pp.\\n1693–1701, 2015.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c64b921f-8a13-43e7-96e8-deed206df27f', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\\nSpanbert: Improving pre-training by representing and predicting spans.\\narXiv preprint arXiv:1907.10529, 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c11363e9-4497-48c3-8b52-14d056b370a7', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nGuillaume Lample and Alexis Conneau.\\nCrosslingual language model pretraining.\\narXiv preprint arXiv:1901.07291, 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='524e3751-c34e-4936-b3f2-ec9a087d410d', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\\nAlbert: A lite bert for self-supervised learning of language representations.\\narXiv preprint arXiv:1909.11942, 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3f26853a-adab-4ac8-acb8-3146074169a4', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\\nThe Winograd schema challenge.\\nIn AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\\n47, 2011.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a5d7ee39-c372-404e-a9ad-8958c1f51875', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nYang Liu and Mirella Lapata.\\nText summarization with pretrained encoders.\\narXiv preprint arXiv:1908.08345, 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f631f49b-b46c-4fa9-9d3b-73341557160e', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\nRoberta: A robustly optimized bert pretraining approach.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da827e30-0d12-46da-8611-9d1bfe23dd01', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\narXiv preprint arXiv:1907.11692, 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a6459e55-4c54-4273-905f-2992ed8b734b', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\\nEfﬁcient estimation of word representations in vector space.\\narXiv preprint arXiv:1301.3781, 2013.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad426bae-5832-4326-8e46-840c0e717426', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\\nDon’t give me the details, just the summary!\\ntopicaware convolutional neural networks for extreme summarization.\\narXiv preprint arXiv:1808.08745, 2018.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0c4303af-98dc-4786-9cbf-6594d070c1af', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nGabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton.\\nRegularizing neural networks by penalizing conﬁdent output distributions.\\narXiv preprint arXiv:1701.06548, 2017.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f029c44c-1421-403d-b9c0-f4c8276f28ce', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\\nDeep contextualized word representations.\\narXiv preprint arXiv:1802.05365, 2018.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b67e4c56-84f2-47a5-9152-fe56cf68cd2e', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\\nImproving language understanding by generative pre-training.\\nURL https://s3-us-west-2.\\namazonaws.\\ncom/openaiassets/researchcovers/languageunsupervised/language understanding paper.\\npdf, 2018.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='183197c6-aaed-4331-ac94-7f1241b94d0f', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\\nLanguage models are unsupervised multitask learners.\\nOpenAI Blog, 1(8), 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a7923bc9-5b06-4d76-a336-9d511a7d4efd', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\\nSquad: 100,000+ questions for machine comprehension of text.\\narXiv preprint arXiv:1606.05250, 2016.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7aae17ee-6eaa-431f-bff2-b6cb0c5d8f17', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nAbigail See, Peter J Liu, and Christopher D Manning.\\nGet to the point: Summarization with pointer-generator networks.\\narXiv preprint arXiv:1704.04368, 2017.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='87aadc60-f277-405e-949e-495a489528a2', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\nEdinburgh neural machine translation systems for WMT 16.\\nIn Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70a781cb-2b2d-4bf1-bc47-9a12d76ea146', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\\nRecursive deep models for semantic compositionality over a sentiment treebank.\\nIn Proceedings of EMNLP, pp.\\n1631–1642, 2013.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3bda264b-a2b3-4fdd-bbfd-7fe2893b16c7', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\\nMass: Masked sequence to sequence pretraining for language generation.\\nIn International Conference on Machine Learning, 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8ac745da-8643-48d1-b8e2-74a860dbd1a0', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.\\nAttention is all you need.\\nIn Advances in neural information processing systems, pp.\\n5998–6008, 2017.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='169f8bf5-305a-4765-87c0-ad9ea310a309', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\\nGlue: A multi-task benchmark and analysis platform for natural language understanding.\\narXiv preprint arXiv:1804.07461, 2018.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='668af35a-5fbe-4179-bf09-443edcb4d00c', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\\nNeural network acceptability judgments.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9594e62e-271f-4b25-8e18-0ebb43c757d1', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\narXiv preprint 1805.12471, 2018.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='467dfce8-93f2-4e36-9f69-2083f60ec0b6', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\\nA broad-coverage challenge corpus for sentence understanding through inference.\\narXiv preprint arXiv:1704.05426, 2017.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2a0f68d-7edf-4f8c-92d2-0abca1d3f627', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\\nA broad-coverage challenge corpus for sentence understanding through inference.\\nIn Proceedings of NAACL-HLT, 2018.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05eed8c2-849d-41ea-a5eb-bf5e7fbea6b5', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\\nXlnet: Generalized autoregressive pretraining for language understanding.\\narXiv preprint arXiv:1906.08237, 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LongRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

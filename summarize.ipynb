{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for bigainlco/LooGLE contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigainlco/LooGLE\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from rag_components import *\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Specific Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks, summarizer_llm=mixtral, text_title=None):\n",
    "    \"\"\"\n",
    "    Generate a summary for each text chunk using the summarizer_llm.\n",
    "\n",
    "    Args:\n",
    "        chunks (list[str]): A list of text chunks to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Summaries of the text chunks.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    if text_title is None:\n",
    "        text_title = \"chunks\"\n",
    "    for chunk in tqdm(chunks, desc=f'Summarizing \"{text_title}\"', leave=False):\n",
    "        if len(summaries) == 0:\n",
    "            prompt = f\"\"\"<s>[INST]Summarize the following text conceptually. \\\n",
    "                The summary should paraphrase the original text, be significantly \\\n",
    "                shorter, retain all propositions, and be able to replace the \\\n",
    "                original text. Here is the text to be summarized:\\n\\n{chunk}[/INST]\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"<s>[INST]Summarize the following text conceptually in the context of the text that precedes it. \\\n",
    "                The summary should paraphrase the original text, be significantly \\\n",
    "                shorter, retain all propositions, and be able to replace the \\\n",
    "                original text. Here is the the context:\\n\\n{summaries[-1]}\\n\\n\\\n",
    "                Summarize the following text:\\n\\n{chunk}[/INST]\"\"\"\n",
    "\n",
    "        prompt = re.sub(r\"\\s+\", \" \", prompt)\n",
    "        response = summarizer_llm.complete(prompt).text.strip()\n",
    "        summaries.append(response)\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def summarize_index(text_title, overwrite_existing=False, summarizer_llm=Settings.llm):\n",
    "    \"\"\"\n",
    "    Generate a summary for each text chunk in the index using the summarizer_llm.\n",
    "\n",
    "    Args:\n",
    "        index (VectorStoreIndex): The index containing text chunks to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: An index containing summaries for each text chunk.\n",
    "    \"\"\"\n",
    "    if not overwrite_existing:\n",
    "        existing_index = get_index_by_title(f\"{text_title}_summaries\")\n",
    "        if existing_index:\n",
    "            return existing_index\n",
    "    index = get_index_by_title(text_title)\n",
    "    text_chunk_ids = get_ids_from_index(index)\n",
    "    id_nums = [int(id_.split(\"_\")[-1]) for id_ in text_chunk_ids]\n",
    "    summary_ids = [f\"summary_{id_num}\" for id_num in id_nums]\n",
    "    chunks = [get_text_by_id(index, node_id) for node_id in text_chunk_ids]\n",
    "    summaries = summarize_chunks(chunks, summarizer_llm, text_title=text_title)\n",
    "\n",
    "    summary_index = create_index_from_chunks_with_ids(\n",
    "        summaries, summary_ids, f\"{text_title}_summaries\", overwrite_existing=overwrite_existing\n",
    "    )\n",
    "    return summary_index\n",
    "\n",
    "\n",
    "def answer_reading_comprehension_with_summarization(\n",
    "    question, context_title, context_text, top_k=2, chunk_size=1024, chunk_overlap=200, qa_llm=gpt4\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question given a context.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_title (str): The title of the context\n",
    "        context_text (str): The text of the context\n",
    "        top_k (int): The number of top matching chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the top k most relevant summary chunks\n",
    "    text_index_title = f\"{context_title}\"\n",
    "    summary_index = summarize_index(text_index_title, summarizer_llm=mixtral)\n",
    "    summary_retriever = VectorIndexRetriever(\n",
    "        index=summary_index,\n",
    "        similarity_top_k=top_k,\n",
    "    )\n",
    "    top_summary_chunks = summary_retriever.retrieve(question)\n",
    "\n",
    "    # Identify the ids of the corresponding raw text chunks\n",
    "    retrieved_summary_ids = [chunk.node.id_ for chunk in top_summary_chunks]\n",
    "    corresponding_text_ids = [f\"text_chunk_{summary_id.split('_')[-1]}\" for summary_id in retrieved_summary_ids]\n",
    "\n",
    "    # Get the correponding raw text chunks\n",
    "    text_index = create_index_from_text_with_ids(\n",
    "        context_text, text_index_title, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    corresonding_chunks = [get_node_by_id(text_index, text_id) for text_id in corresponding_text_ids]\n",
    "\n",
    "    # Find the top k * 10 most relevant raw text chunks, and try to find the rank of the chunks which correspond to the top summary chunks\n",
    "    text_retriever = VectorIndexRetriever(\n",
    "        index=text_index,\n",
    "        similarity_top_k=top_k * 10,\n",
    "    )\n",
    "    retrieved_texts = text_retriever.retrieve(question)\n",
    "    text_chunk_ranks = []\n",
    "    for i, chunk in enumerate(corresonding_chunks):\n",
    "        id_ = chunk.id_\n",
    "        rank = None\n",
    "        for j, retrieved_text in enumerate(retrieved_texts):\n",
    "            if retrieved_text.node.id_ == id_:\n",
    "                rank = j\n",
    "                break\n",
    "        text_chunk_ranks.append(rank)\n",
    "\n",
    "    corresponding_chunks_text = [chunk.text for chunk in corresonding_chunks]\n",
    "    corresponding_chunks_text_combined = \" \".join(corresponding_chunks_text)\n",
    "\n",
    "    response = answer_reading_comprehension(question, corresponding_chunks_text_combined, qa_llm=qa_llm)\n",
    "\n",
    "    top_chunks_info = [\n",
    "        {\n",
    "            \"summary_score\": top_summary_chunks[i].score,\n",
    "            \"summary_rank\": i,\n",
    "            \"text_score\": similarity_score(question, text_index_title, corresonding_chunks[i].id_),\n",
    "            \"text_rank\": text_chunk_ranks[i],\n",
    "            \"summary\": top_summary_chunks[i].node.text,\n",
    "            \"text\": corresonding_chunks[i].text,\n",
    "        }\n",
    "        for i in range(len(corresonding_chunks))\n",
    "    ]\n",
    "\n",
    "    additional_info = {\n",
    "        \"top_chunks_info\": top_chunks_info,\n",
    "    }\n",
    "    return response, additional_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output for chunk size 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dd5017c29d434db4fcce8bad7497a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Answering questions:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aebd50b63724e608edda4da2f141788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"Claude List_chunksize64\":   0%|          | 0/364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e3501214bc490d90881fb4c3188bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"Climate change in Washington (state)_chunksize64\":   0%|          | 0/364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124ec8f943464d00a63196c81e62f37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"Execution of Nagaenthran K. Dharmalingam_chunksize64\":   0%|          | 0/292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e4ffabc02a4778b58b461e62409e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"Foreign Cattle Market_chunksize64\":   0%|          | 0/318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe2b1bbe63c4116b91c61f3b5aef16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"Governorship of Glenn Youngkin_chunksize64\":   0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97303ab4c7d343638be4fc05d94443e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"Hells Angels MC criminal allegations and incidents in the United States_chunksize64\":   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e060684a022943e38a7821409a191426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"History of NBC_chunksize64\":   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87a6df36f2b489eb1dc965103a8084c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"Light in painting_chunksize64\":   0%|          | 0/1092 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa9f4b06ddb443e96b7a4db4aa197c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"Nightlife in Belgrade_chunksize64\":   0%|          | 0/496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa17dd17691148a080f5b6e1ef024b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"Police brutality by country_chunksize64\":   0%|          | 0/535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output for chunk size 128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4dba8aa37c4989b9f0f3d3999ae9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Answering questions:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d0d3099e9240639133bffb32880429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing \"José Luis Picardo_chunksize128\":   0%|          | 0/158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'vector_store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating output for chunk size\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunk_sizes[idx])\n\u001b[1;32m      7\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/summarization_with_corresponding_in_context_mistral_large_chunksize\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_sizes[idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtest_longdep_qa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer_reading_comprehension_with_summarization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug_lim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_lim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqa_llm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmistral_large\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopks\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_overlaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Advanced Deep Learning/LongRAG/rag_components.py:298\u001b[0m, in \u001b[0;36mtest_longdep_qa\u001b[0;34m(inference_function, output_file, debug_lim, qa_llm, chunk_size, top_k, chunk_overlap)\u001b[0m\n\u001b[1;32m    296\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m question_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m question_is_answered(question, existing_output):\n\u001b[0;32m--> 298\u001b[0m     inference_response \u001b[38;5;241m=\u001b[39m \u001b[43minference_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqa_llm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqa_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inference_response, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    308\u001b[0m         generated_answer \u001b[38;5;241m=\u001b[39m inference_response[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 116\u001b[0m, in \u001b[0;36manswer_reading_comprehension_with_summarization\u001b[0;34m(question, context_title, context_text, top_k, chunk_size, chunk_overlap, qa_llm)\u001b[0m\n\u001b[1;32m    112\u001b[0m corresponding_chunks_text_combined \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corresponding_chunks_text)\n\u001b[1;32m    114\u001b[0m response \u001b[38;5;241m=\u001b[39m answer_reading_comprehension(question, corresponding_chunks_text_combined, qa_llm\u001b[38;5;241m=\u001b[39mqa_llm)\n\u001b[0;32m--> 116\u001b[0m top_chunks_info \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    117\u001b[0m     {\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_summary_chunks[i]\u001b[38;5;241m.\u001b[39mscore,\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_rank\u001b[39m\u001b[38;5;124m\"\u001b[39m: i,\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: similarity_score(question, text_index_title, corresonding_chunks[i]\u001b[38;5;241m.\u001b[39mid_),\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_rank\u001b[39m\u001b[38;5;124m\"\u001b[39m: text_chunk_ranks[i],\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_summary_chunks[i]\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: corresonding_chunks[i]\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m    124\u001b[0m     }\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corresonding_chunks))\n\u001b[1;32m    126\u001b[0m ]\n\u001b[1;32m    128\u001b[0m additional_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_chunks_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_chunks_info,\n\u001b[1;32m    130\u001b[0m }\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response, additional_info\n",
      "Cell \u001b[0;32mIn[2], line 120\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m corresponding_chunks_text_combined \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corresponding_chunks_text)\n\u001b[1;32m    114\u001b[0m response \u001b[38;5;241m=\u001b[39m answer_reading_comprehension(question, corresponding_chunks_text_combined, qa_llm\u001b[38;5;241m=\u001b[39mqa_llm)\n\u001b[1;32m    116\u001b[0m top_chunks_info \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    117\u001b[0m     {\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_summary_chunks[i]\u001b[38;5;241m.\u001b[39mscore,\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_rank\u001b[39m\u001b[38;5;124m\"\u001b[39m: i,\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43msimilarity_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_index_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorresonding_chunks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid_\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_rank\u001b[39m\u001b[38;5;124m\"\u001b[39m: text_chunk_ranks[i],\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_summary_chunks[i]\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: corresonding_chunks[i]\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m    124\u001b[0m     }\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corresonding_chunks))\n\u001b[1;32m    126\u001b[0m ]\n\u001b[1;32m    128\u001b[0m additional_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_chunks_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_chunks_info,\n\u001b[1;32m    130\u001b[0m }\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response, additional_info\n",
      "File \u001b[0;32m~/Desktop/Advanced Deep Learning/LongRAG/rag_components.py:47\u001b[0m, in \u001b[0;36msimilarity_score\u001b[0;34m(query, index_id, node_id)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_score\u001b[39m(query, index_id, node_id):\n\u001b[1;32m     46\u001b[0m     index \u001b[38;5;241m=\u001b[39m get_index_by_title(index_id)\n\u001b[0;32m---> 47\u001b[0m     node_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_store\u001b[49m\u001b[38;5;241m.\u001b[39mget(node_id))\n\u001b[1;32m     48\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Settings\u001b[38;5;241m.\u001b[39membed_model\u001b[38;5;241m.\u001b[39m_get_query_embedding(query))\n\u001b[1;32m     49\u001b[0m     score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(node_embedding, query_embedding)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'vector_store'"
     ]
    }
   ],
   "source": [
    "debug_lim = 100\n",
    "chunk_sizes = [64, 128, 256, 512, 1024, 2048]\n",
    "topks = [32, 16, 8, 4, 2, 1]\n",
    "chunk_overlaps = [10, 25, 50, 100, 200, 400]\n",
    "for idx in [1, 2, 4, 5]:\n",
    "    print(\"Generating output for chunk size\", chunk_sizes[idx])\n",
    "    output_file = f\"output/summarization_with_corresponding_in_context_mistral_large_chunksize{chunk_sizes[idx]}.jsonl\"\n",
    "    test_longdep_qa(\n",
    "        answer_reading_comprehension_with_summarization,\n",
    "        output_file=output_file,\n",
    "        debug_lim=debug_lim,\n",
    "        qa_llm=mistral_large,\n",
    "        chunk_size=chunk_sizes[idx],\n",
    "        top_k=topks[idx],\n",
    "        chunk_overlap=chunk_overlaps[idx],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for chunk size 64\n",
      "Rouge Metrics: {'rouge1': 0.21461251375303753, 'rouge2': 0.08277143946620485, 'rougeL': 0.16944911367801682, 'rougeLsum': 0.17812366920734976}\n",
      "LLM Self-Score: 0.48\n",
      "results for chunk size 512\n",
      "Rouge Metrics: {'rouge1': 0.22652400729944394, 'rouge2': 0.0992675643342062, 'rougeL': 0.1763018316625178, 'rougeLsum': 0.18412526838410892}\n",
      "LLM Self-Score: 0.43\n"
     ]
    }
   ],
   "source": [
    "for idx in [0, 3]:\n",
    "    output_file = f\"output/summarization_with_corresponding_in_context_mistral_large_chunksize{chunk_sizes[idx]}.jsonl\"\n",
    "    rouge_metrics = get_rouge_metrics(output_file)\n",
    "    print(\"results for chunk size\", chunk_sizes[idx])\n",
    "    print(\"Rouge Metrics:\", rouge_metrics)\n",
    "\n",
    "    self_score = llm_self_score(output_file, llm=gpt4)\n",
    "    print(\"LLM Self-Score:\", self_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LongRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

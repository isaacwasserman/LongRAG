{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from rag_components import *\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from rich.progress import track\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposition Specific Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_propositions_from_chunks(chunks, proposition_llm=Settings.llm, text_title=None):\n",
    "    \"\"\"\n",
    "    Generate propositions for each text chunk using the proposition_llm.\n",
    "\n",
    "    Args:\n",
    "        chunks (list[str]): A list of text chunks to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Summaries of the text chunks.\n",
    "    \"\"\"\n",
    "    proposition_sets = []\n",
    "    if text_title is None:\n",
    "        text_title = \"chunks\"\n",
    "    for chunk in tqdm(chunks, desc=f'Generating propositions for \"{text_title}\"', leave=False):\n",
    "        prompt = f\"\"\"\\\n",
    "            {chunk}\n",
    "\n",
    "            Decompose the text above into a set of clear and simple propositions, ensuring they are interpretable out of context.\n",
    "            1. Split compound sentences into simple sentences. Maintain the original phrasing from the input whenever possible.\n",
    "            2. For any named entity that is accompanied by additional descriptive information, separate this information into \\\n",
    "            its own distinct proposition.\n",
    "            3. Decontextualize the proposition by adding necessary modifier to nouns or entire sentences and replacing pronouns \\\n",
    "            (e.g., \"it\", \"he\", \"she\", \"they\", \"this\", \"that\") with the full name of the entities they refer to.\n",
    "            4. Present the results as a numbered list of propositions, each on a new line and prefixed with the proposition number.\n",
    "\n",
    "            Propositions: \\\n",
    "        \"\"\"\n",
    "        prompt = re.sub(r\" +\", \" \", prompt)\n",
    "        proposition_set_raw = proposition_llm.complete(prompt).text.strip()\n",
    "        proposition_set = proposition_set_raw.split(\"\\n\")\n",
    "        proposition_set = [re.sub(r\"^\\d+\\.\", \"\", proposition) for proposition in proposition_set]\n",
    "        proposition_set = [proposition.strip() for proposition in proposition_set]\n",
    "        proposition_set = [proposition for proposition in proposition_set if proposition]\n",
    "        proposition_sets.append(proposition_set)\n",
    "    return proposition_sets\n",
    "\n",
    "\n",
    "def generate_propositions_from_index(text_title, overwrite_existing=False, proposition_llm=Settings.llm):\n",
    "    \"\"\"\n",
    "    Generate propositions for each text chunk in the index using GPT-4.\n",
    "\n",
    "    Args:\n",
    "        index (VectorStoreIndex): The index containing text chunks for which to generate propositions.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: An index containing the propositions generated for each text chunk.\n",
    "    \"\"\"\n",
    "    if not overwrite_existing:\n",
    "        existing_index = get_index_by_title(f\"{text_title}_propositions\")\n",
    "        if existing_index:\n",
    "            return existing_index\n",
    "    index = get_index_by_title(text_title)\n",
    "    text_chunk_ids = get_ids_from_index(index)\n",
    "\n",
    "    chunks = [get_text_by_id(index, node_id) for node_id in text_chunk_ids]\n",
    "    proposition_sets = generate_propositions_from_chunks(chunks, proposition_llm, text_title=text_title)\n",
    "    propositions = [proposition for proposition_set in proposition_sets for proposition in proposition_set]\n",
    "\n",
    "    id_nums = [\n",
    "        [f\"{int(text_chunk_ids[i].split('_')[-1])}_{j}\" for j in range(len(proposition_sets[i]))]\n",
    "        for i in range(len(proposition_sets))\n",
    "    ]\n",
    "    id_nums = [id_num for id_num_set in id_nums for id_num in id_num_set]\n",
    "    proposition_ids = [f\"proposition_{id_num}\" for id_num in id_nums]\n",
    "\n",
    "    proposition_index = create_index_from_chunks_with_ids(\n",
    "        propositions, proposition_ids, f\"{text_title}_propositions\", overwrite_existing=overwrite_existing\n",
    "    )\n",
    "    return proposition_index\n",
    "\n",
    "\n",
    "def answer_reading_comprehension_with_propositions(\n",
    "    question, context_title, context_text, top_k=2, chunk_size=1024, chunk_overlap=200, qa_llm=gpt4\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question given a context, using proposition generation.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_title (str): The title of the context\n",
    "        context_text (str): The text of the context\n",
    "        top_k (int): The number of top matching chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the top k most relevant proposition chunks\n",
    "    proposition_index = generate_propositions_from_index(context_title, proposition_llm=mixtral)\n",
    "    proposition_retriever = VectorIndexRetriever(\n",
    "        index=proposition_index,\n",
    "        similarity_top_k=top_k,\n",
    "    )\n",
    "    top_proposition_chunks = proposition_retriever.retrieve(question)\n",
    "\n",
    "    # Identify the ids of the corresponding raw text chunks\n",
    "    retrieved_proposition_ids = [chunk.node.id_ for chunk in top_proposition_chunks]\n",
    "    corresponding_text_ids = [f\"text_chunk_{proposition_id.split('_')[-2]}\" for proposition_id in retrieved_proposition_ids]\n",
    "\n",
    "    # Get the correponding raw text chunks\n",
    "    text_index = create_index_from_text_with_ids(\n",
    "        context_text, context_title, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    corresonding_chunks = [get_node_by_id(text_index, text_id) for text_id in corresponding_text_ids]\n",
    "\n",
    "    # Find the top k * 10 most relevant raw text chunks, and try to find the rank of the chunks which correspond to the top proposition chunks\n",
    "    text_retriever = VectorIndexRetriever(\n",
    "        index=text_index,\n",
    "        similarity_top_k=top_k * 10,\n",
    "    )\n",
    "    retrieved_texts = text_retriever.retrieve(question)\n",
    "    text_chunk_ranks = []\n",
    "    for i, chunk in enumerate(corresonding_chunks):\n",
    "        id_ = chunk.id_\n",
    "        rank = None\n",
    "        for j, retrieved_text in enumerate(retrieved_texts):\n",
    "            if retrieved_text.node.id_ == id_:\n",
    "                rank = j\n",
    "                break\n",
    "        text_chunk_ranks.append(rank)\n",
    "\n",
    "    corresponding_chunks_text = [chunk.text for chunk in corresonding_chunks]\n",
    "    corresponding_chunks_text_combined = \" \".join(corresponding_chunks_text)\n",
    "\n",
    "    prompt = f\"\"\"Consider the following context with depth and thoughtfulness: {corresponding_chunks_text_combined}\\n\\n\\\n",
    "        Respond to the following question with insight and nuance. Answer concisely, often in one \\\n",
    "        sentence or less and sometimes in the form of a list or structured text. If the question \\\n",
    "        asks you to order events, refer to the events by their number (e.g. \"1. third event, 2. second \\\n",
    "        event, 3. first event\" -> \"3, 2, 1\"). Answer multiple choice questions using the number which \\\n",
    "        corresponds to the correct answer (e.g. \"1. A, 2. B, 3. C\" -> \"2\"). Do not include the \\\n",
    "        question in your answer. \\\n",
    "        \\n\\n\\\n",
    "        Question: {question}\\n\\n\\\n",
    "        Answer: Considering the context above, \"\"\"\n",
    "    response = qa_llm.complete(prompt).text\n",
    "\n",
    "    top_chunks_info = [\n",
    "        {\n",
    "            \"summary_score\": top_proposition_chunks[i].score,\n",
    "            \"proposition_rank\": i,\n",
    "            \"text_score\": similarity_score(question, context_title, corresonding_chunks[i].id_),\n",
    "            \"text_rank\": text_chunk_ranks[i],\n",
    "            \"proposition\": top_proposition_chunks[i].node.text,\n",
    "            \"text\": corresonding_chunks[i].text,\n",
    "        }\n",
    "        for i in range(len(corresonding_chunks))\n",
    "    ]\n",
    "\n",
    "    additional_info = {\n",
    "        \"top_chunks_info\": top_chunks_info,\n",
    "    }\n",
    "    return response, additional_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8115f50694f64992967737993a819d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Answering questions:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40d78ac5ea34bca8c987696796aa844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"2023 French pension reform unrest\":   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79062aca37544935803d2cca1b4b0ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"2023 Turkey–Syria earthquake\":   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bb687f90784363b0043069ef8dbe01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"Claude List\":   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccf58fdf36e48d8964c1d4a2d2363a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"Climate change in Washington (state)\":   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db488cb21ad47049a69dfdf11cdee89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"Execution of Nagaenthran K. Dharmalingam\":   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5f1209f5a441c8baf2573877785dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"Foreign Cattle Market\":   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6a7153f8d24a6a94d6cc872fe3bbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"Governorship of Glenn Youngkin\":   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0175d3a1383485da41ade9cd86a50c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"Hells Angels MC criminal allegations and incidents in the United States\":   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e4341887964211afcb7499077d6bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"History of NBC\":   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abfa698812741138d997b983707304c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"Light in painting\":   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df806ff9eab4ca3a3c7391bc1e5365a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"Nightlife in Belgrade\":   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e88d818d174d1885a0c17f9a401518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating propositions for \"Police brutality by country\":   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "debug_lim = 100\n",
    "output_file = \"output/propositions_with_corresponding_in_context.jsonl\"\n",
    "test_longdep_qa(answer_reading_comprehension_with_propositions, output_file=output_file, debug_lim=debug_lim, qa_llm=gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge Metrics: {'rouge1': 0.40281104491103636, 'rouge2': 0.17082979780142077, 'rougeL': 0.35271561346205105, 'rougeLsum': 0.3564499653389325}\n",
      "LLM Self-Score: 0.38\n"
     ]
    }
   ],
   "source": [
    "rouge_metrics = get_rouge_metrics(output_file)\n",
    "print(\"Rouge Metrics:\", rouge_metrics)\n",
    "\n",
    "self_score = llm_self_score(output_file, llm=gpt4)\n",
    "print(\"LLM Self-Score:\", self_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LongRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

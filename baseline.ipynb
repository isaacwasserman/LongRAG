{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "import shutil\n",
    "import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0.2, model=\"gpt-4\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_from_text(text, title):\n",
    "    \"\"\"\n",
    "    Create an index from a piece of context. If an index with the given title already exists, it will be returned.\n",
    "\n",
    "    Args:\n",
    "        text (str): The context to index\n",
    "        title (str): The title of the index for access later\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The index created from the context\n",
    "    \"\"\"\n",
    "    index = get_index_by_title(title)\n",
    "    if index is None:\n",
    "        os.makedirs(\"tmp\", exist_ok=True)\n",
    "        with open(\"tmp/tmp.txt\", \"w\") as f:\n",
    "            f.write(text)\n",
    "        documents = SimpleDirectoryReader(\"tmp\").load_data()\n",
    "        index = VectorStoreIndex.from_documents(documents, model_name=\"openai/text-embedding-3-small\")\n",
    "        index.set_index_id(title)\n",
    "        index.storage_context.persist()\n",
    "        shutil.rmtree(\"tmp\")\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_index_by_title(title):\n",
    "    \"\"\"\n",
    "    Get an index by its title. If the index does not exist, returns None.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the index\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The index with the specified title\n",
    "    \"\"\"\n",
    "    try:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "        index = load_index_from_storage(storage_context, index_id=title)\n",
    "        return index\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_reading_comprehension(question, context_title=None, context=\"\", use_rag=False):\n",
    "    \"\"\"\n",
    "    Answer a question given a context. If use_rag is True, retrieval will be used to answer the question. Otherwise, the entire context will be prepended to the question.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_title (str): The title of the context index\n",
    "        context (str): The context to use\n",
    "        use_rag (bool): Whether to use retrieval to answer the question\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "    if use_rag:\n",
    "        if context_title is None:\n",
    "            raise ValueError(\"context_title must be provided when using RAG\")\n",
    "        index = create_index_from_text(context, context_title)\n",
    "        query_engine = index.as_query_engine()\n",
    "        response = query_engine.query(question).response\n",
    "    else:\n",
    "        response = Settings.llm.complete(context + \"\\n\" + question).text\n",
    "    return response\n",
    "\n",
    "\n",
    "def answer_reading_comprehension_with_rag(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Answer a question given a context using retrieval to answer the question.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_title (str): The title of the context index\n",
    "        context (str): The context to use\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "    return answer_reading_comprehension(*args, **kwargs, use_rag=True)\n",
    "\n",
    "\n",
    "def answer_reading_comprehension_in_context(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Answer a question given a context. The context will be prepended to the question.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context (str): The context to use\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "    return answer_reading_comprehension(*args, **kwargs, use_rag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for bigainlco/LooGLE contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigainlco/LooGLE\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "longdep_qa_ds = load_dataset(\"bigainlco/LooGLE\", \"longdep_qa\", split=\"test\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def get_rouge_metrics(output_file):\n",
    "    \"\"\"\n",
    "    Get ROUGE metrics for a .jsonl file containing generated answers and ground truth answers.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): The path to the .jsonl file\n",
    "\n",
    "    Returns:\n",
    "        dict: The ROUGE metrics\n",
    "    \"\"\"\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    outputs = [json.loads(line) for line in lines]\n",
    "    generated_answers = [output[\"generated_answer\"] for output in outputs]\n",
    "    ground_truths = [output[\"ground_truth\"] for output in outputs]\n",
    "    rouge_metrics = rouge.compute(predictions=generated_answers, references=ground_truths)\n",
    "    return rouge_metrics\n",
    "\n",
    "\n",
    "def llm_self_score(output_file):\n",
    "    \"\"\"\n",
    "    Score the generated answers in a .jsonl file using the LLM. The user will be prompted to determine whether each answer is correct.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): The path to the .jsonl file\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the generated answers\n",
    "    \"\"\"\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    outputs = [json.loads(line) for line in lines]\n",
    "    llm = Settings.llm\n",
    "    for output in outputs:\n",
    "        question = output[\"question\"]\n",
    "        ground_truth = output[\"ground_truth\"]\n",
    "        generated_answer = output[\"generated_answer\"]\n",
    "        if \"correct\" in output:\n",
    "            continue\n",
    "        prompt = f'Given the question \"{question}\" whose answer is \"{ground_truth}\", is answer \"{generated_answer}\" similar enough to the true answer that it should be considered correct? Answer \"yes\" or \"no\" with no other characters or capitalization.'\n",
    "        response = llm.complete(prompt).text.lower()\n",
    "        if \"yes\" in response:\n",
    "            output[\"correct\"] = True\n",
    "        else:\n",
    "            output[\"correct\"] = False\n",
    "    num_correct = sum([output[\"correct\"] for output in outputs])\n",
    "    accuracy = num_correct / len(outputs)\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for output in outputs:\n",
    "            json.dump(output, f)\n",
    "            f.write(\"\\n\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference function and helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_output_file(output_file):\n",
    "    \"\"\"\n",
    "    Read a .jsonl file containing generated answers and ground truth answers.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): The path to the .jsonl file\n",
    "\n",
    "    Returns:\n",
    "        list: The outputs in the file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_file):\n",
    "        return []\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    outputs = [json.loads(line) for line in lines]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def log_outputs(question, ground_truth, generated_answer, output_file):\n",
    "    \"\"\"\n",
    "    Log a question, its ground truth answer, and a generated answer to a .jsonl file.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question\n",
    "        ground_truth (str): The ground truth answer\n",
    "        generated_answer (str): The generated answer\n",
    "        output_file (str): The path to the .jsonl file; if None, a new file will be created in the \"output\" directory with the current time as the name\n",
    "\n",
    "    Returns:\n",
    "        tuple: The path to the .jsonl file and the outputs in the file\n",
    "    \"\"\"\n",
    "    if output_file is None:\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        output_file = f\"output/{time.time()}.jsonl\"\n",
    "    with open(output_file, \"a\") as f:\n",
    "        json.dump({\"question\": question, \"ground_truth\": ground_truth, \"generated_answer\": generated_answer}, f)\n",
    "        f.write(\"\\n\")\n",
    "    existing_output = read_output_file(output_file)\n",
    "    return output_file, existing_output\n",
    "\n",
    "\n",
    "def question_is_answered(question, existing_output):\n",
    "    \"\"\"\n",
    "    Determine whether a question has already been answered in a list of outputs.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question\n",
    "        existing_output (list): The outputs\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the question has already been answered\n",
    "    \"\"\"\n",
    "    if question in [output[\"question\"] for output in existing_output]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def test_longdep_qa(inference_function, output_file=None, debug_lim=None):\n",
    "    \"\"\"\n",
    "    Test an inference function on the longdep_qa dataset.\n",
    "\n",
    "    Args:\n",
    "        inference_function (function): The function to test\n",
    "        output_file (str): The path to the .jsonl file to log outputs to; if None, a new file will be created in the \"output\" directory with the current time as the name\n",
    "        debug_lim (int): The number of questions to test; if None, all questions will be tested\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    n_questions = sum([len(eval(env[\"qa_pairs\"])) for env in longdep_qa_ds])\n",
    "    if debug_lim is None:\n",
    "        debug_lim = n_questions\n",
    "    existing_output = read_output_file(output_file)\n",
    "    with tqdm.tqdm(total=debug_lim) as pbar:\n",
    "        for environment in longdep_qa_ds:\n",
    "            context = environment[\"input\"]\n",
    "            title = environment[\"title\"]\n",
    "            qa_pairs = eval(environment[\"qa_pairs\"])\n",
    "            for question_dict in qa_pairs:\n",
    "                question = question_dict[\"Q\"]\n",
    "                ground_truth = question_dict[\"A\"]\n",
    "                if not question_is_answered(question, existing_output):\n",
    "                    generated_answer = inference_function(question, context_title=title, context=context)\n",
    "                    output_file, existing_output = log_outputs(question, ground_truth, generated_answer, output_file)\n",
    "                pbar.update(1)\n",
    "                if pbar.n >= debug_lim:\n",
    "                    break\n",
    "            if pbar.n >= debug_lim:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 22030.06it/s]\n"
     ]
    }
   ],
   "source": [
    "test_longdep_qa(answer_reading_comprehension_with_rag, output_file=\"output/baseline_with_rag_100.jsonl\", debug_lim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge Metrics: {'rouge1': 0.19845643848785122, 'rouge2': 0.07862381462209828, 'rougeL': 0.1619569034833745, 'rougeLsum': 0.17183036257033626}\n",
      "LLM Self-Score: 0.53\n"
     ]
    }
   ],
   "source": [
    "rouge_metrics = get_rouge_metrics(\"output/baseline_with_rag_100.jsonl\")\n",
    "print(\"Rouge Metrics:\", rouge_metrics)\n",
    "\n",
    "llm_self_score = llm_self_score(\"output/baseline_with_rag_100.jsonl\")\n",
    "print(\"LLM Self-Score:\", llm_self_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LongRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for bigainlco/LooGLE contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigainlco/LooGLE\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from rag_components import *\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation Specific Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_chunks(chunks, question_gen_llm=Settings.llm, text_title=None, num_questions_per_1k_tokens=32):\n",
    "    \"\"\"\n",
    "    Generate questions for each text chunk using the question_gen_llm.\n",
    "\n",
    "    Args:\n",
    "        chunks (list[str]): A list of text chunks to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Summaries of the text chunks.\n",
    "    \"\"\"\n",
    "    question_sets = []\n",
    "    if text_title is None:\n",
    "        text_title = \"chunks\"\n",
    "    for chunk in tqdm(chunks, desc=f'Generating questions for \"{text_title}\"', leave=False):\n",
    "        num_questions = round(count_tokens(chunk) * num_questions_per_1k_tokens / 1024)\n",
    "        prompt = f\"\"\"\\\n",
    "            <s>[INST]{chunk}\n",
    "\n",
    "            Generate {num_questions} reading comprehension questions based on the text above. \\\n",
    "            Each question should be unique and should require a thoughtful answer. \\\n",
    "            Each question should begin with a number followed by a period and a space, denoting its position in the list. \\\n",
    "            Separate each question with a line break. [/INST]\\\n",
    "\n",
    "            Questions: \\\n",
    "        \"\"\"\n",
    "        prompt = re.sub(r\" +\", \" \", prompt)\n",
    "        question_set_raw = question_gen_llm.complete(prompt).text.strip()\n",
    "        question_set = question_set_raw.split(\"\\n\")\n",
    "        question_set = [re.sub(r\"^\\d+\\.\", \"\", question) for question in question_set]\n",
    "        question_set = [question.strip() for question in question_set]\n",
    "        question_set = [question for question in question_set if question]\n",
    "        question_sets.append(question_set)\n",
    "    return question_sets\n",
    "\n",
    "\n",
    "def generate_questions_from_index(text_title, overwrite_existing=False, question_gen_llm=Settings.llm):\n",
    "    \"\"\"\n",
    "    Generate questions for each text chunk in the index using GPT-4.\n",
    "\n",
    "    Args:\n",
    "        index (VectorStoreIndex): The index containing text chunks for which to generate questions.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: An index containing the questions generated for each text chunk.\n",
    "    \"\"\"\n",
    "    if not overwrite_existing:\n",
    "        existing_index = get_index_by_title(f\"{text_title}_questions\")\n",
    "        if existing_index:\n",
    "            return existing_index\n",
    "    index = get_index_by_title(text_title)\n",
    "    text_chunk_ids = get_ids_from_index(index)\n",
    "\n",
    "    chunks = [get_text_by_id(index, node_id) for node_id in text_chunk_ids]\n",
    "    question_sets = generate_questions_from_chunks(chunks, question_gen_llm, text_title=text_title)\n",
    "    questions = [question for question_set in question_sets for question in question_set]\n",
    "\n",
    "    id_nums = [\n",
    "        [f\"{int(text_chunk_ids[i].split('_')[-1])}_{j}\" for j in range(len(question_sets[i]))]\n",
    "        for i in range(len(question_sets))\n",
    "    ]\n",
    "    id_nums = [id_num for id_num_set in id_nums for id_num in id_num_set]\n",
    "    question_ids = [f\"question_{id_num}\" for id_num in id_nums]\n",
    "\n",
    "    question_index = create_index_from_chunks_with_ids(\n",
    "        questions, question_ids, f\"{text_title}_questions\", overwrite_existing=overwrite_existing\n",
    "    )\n",
    "    return question_index\n",
    "\n",
    "\n",
    "def answer_reading_comprehension_with_question_generation(\n",
    "    question, context_title, context_text, top_k=2, chunk_size=1024, chunk_overlap=200, qa_llm=gpt4\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question given a context, using question generation.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_title (str): The title of the context\n",
    "        context_text (str): The text of the context\n",
    "        top_k (int): The number of top matching chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the top k most relevant summary chunks\n",
    "    question_index = generate_questions_from_index(context_title, question_gen_llm=mixtral)\n",
    "    question_retriever = VectorIndexRetriever(\n",
    "        index=question_index,\n",
    "        similarity_top_k=top_k * 32,\n",
    "    )\n",
    "    top_question_chunks = question_retriever.retrieve(question)\n",
    "\n",
    "    # Identify the ids of the corresponding raw text chunks\n",
    "    retrieved_question_ids = [chunk.node.id_ for chunk in top_question_chunks]\n",
    "    corresponding_text_ids = [f\"text_chunk_{question_id.split('_')[-2]}\" for question_id in retrieved_question_ids]\n",
    "    unique_corresponding_text_ids = []\n",
    "    for text_id in corresponding_text_ids:\n",
    "        if text_id not in unique_corresponding_text_ids:\n",
    "            unique_corresponding_text_ids.append(text_id)\n",
    "    corresponding_text_ids = unique_corresponding_text_ids[:top_k]\n",
    "\n",
    "    # Get the correponding raw text chunks\n",
    "    text_index = create_index_from_text_with_ids(\n",
    "        context_text, context_title, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    corresonding_chunks = [get_node_by_id(text_index, text_id) for text_id in corresponding_text_ids]\n",
    "\n",
    "    # Find the top k * 10 most relevant raw text chunks, and try to find the rank of the chunks which correspond to the top summary chunks\n",
    "    text_retriever = VectorIndexRetriever(\n",
    "        index=text_index,\n",
    "        similarity_top_k=top_k * 10,\n",
    "    )\n",
    "    retrieved_texts = text_retriever.retrieve(question)\n",
    "    text_chunk_ranks = []\n",
    "    for i, chunk in enumerate(corresonding_chunks):\n",
    "        id_ = chunk.id_\n",
    "        rank = None\n",
    "        for j, retrieved_text in enumerate(retrieved_texts):\n",
    "            if retrieved_text.node.id_ == id_:\n",
    "                rank = j\n",
    "                break\n",
    "        text_chunk_ranks.append(rank)\n",
    "\n",
    "    corresponding_chunks_text = [chunk.text for chunk in corresonding_chunks]\n",
    "    corresponding_chunks_text_combined = \" \".join(corresponding_chunks_text)\n",
    "\n",
    "    response = answer_reading_comprehension(question, corresponding_chunks_text_combined, qa_llm=qa_llm)\n",
    "\n",
    "    top_chunks_info = [\n",
    "        {\n",
    "            \"summary_score\": top_question_chunks[i].score,\n",
    "            \"generated_question_rank\": i,\n",
    "            \"text_score\": similarity_score(question, context_title, corresonding_chunks[i].id_),\n",
    "            \"text_rank\": text_chunk_ranks[i],\n",
    "            \"generated_question\": top_question_chunks[i].node.text,\n",
    "            \"text\": corresonding_chunks[i].text,\n",
    "        }\n",
    "        for i in range(len(corresonding_chunks))\n",
    "    ]\n",
    "\n",
    "    additional_info = {\n",
    "        \"top_chunks_info\": top_chunks_info,\n",
    "    }\n",
    "    return response, additional_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output for chunk size 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337c3ae09fc4414a9f13ff658a32a052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Answering questions:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2601bdbcf0499f9215b484b93e5335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating questions for \"History of NBC_chunksize64\":   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb50b6c86c74a2cafcdc1b9ecd3acb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating questions for \"Light in painting_chunksize64\":   0%|          | 0/1092 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d1df6abf544b5f8b424ed32f6fd1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating questions for \"Nightlife in Belgrade_chunksize64\":   0%|          | 0/496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c76b328f2e4ab9badc617c230b0fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating questions for \"Police brutality by country_chunksize64\":   0%|          | 0/535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "debug_lim = 100\n",
    "chunk_sizes = [64, 128, 256, 512, 1024, 2048]\n",
    "topks = [32, 16, 8, 4, 2, 1]\n",
    "chunk_overlaps = [10, 25, 50, 100, 200, 400]\n",
    "for idx in [0]:\n",
    "    print(\"Generating output for chunk size\", chunk_sizes[idx])\n",
    "    output_file = f\"output/question_generation_with_corresponding_in_context_mistral_large_chunksize{chunk_sizes[idx]}.jsonl\"\n",
    "    test_longdep_qa(\n",
    "        answer_reading_comprehension_with_question_generation,\n",
    "        output_file=output_file,\n",
    "        debug_lim=debug_lim,\n",
    "        qa_llm=mistral_large,\n",
    "        chunk_size=chunk_sizes[idx],\n",
    "        top_k=topks[idx],\n",
    "        chunk_overlap=chunk_overlaps[idx],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for chunk size 64\n",
      "Rouge Metrics: {'rouge1': 0.21558453552561538, 'rouge2': 0.08741380340189148, 'rougeL': 0.17300403529370378, 'rougeLsum': 0.1818136268995544}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07ece08008140cc99e0aac4e86da898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Self-Score: 0.46\n",
      "results for chunk size 128\n",
      "Rouge Metrics: {'rouge1': 0.21951361279946763, 'rouge2': 0.08997997724957946, 'rougeL': 0.18095638821803084, 'rougeLsum': 0.1893749618395414}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3840c186503545d2b36a528e6e33561c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Self-Score: 0.46\n",
      "results for chunk size 256\n",
      "Rouge Metrics: {'rouge1': 0.2658157565644904, 'rouge2': 0.11572563648251799, 'rougeL': 0.20853898400110943, 'rougeLsum': 0.21708856922115966}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c73cd5f6f8a438aac9770e1fd318121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Self-Score: 0.44\n",
      "results for chunk size 512\n",
      "Rouge Metrics: {'rouge1': 0.21550320581484034, 'rouge2': 0.08900312811355401, 'rougeL': 0.18085639365079798, 'rougeLsum': 0.18595131289674927}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c71a750976479a9094c35b8cb04154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Self-Score: 0.46\n",
      "results for chunk size 1024\n",
      "Rouge Metrics: {'rouge1': 0.1996121960974025, 'rouge2': 0.08982223636064446, 'rougeL': 0.16673972667311232, 'rougeLsum': 0.1719032707980939}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731141a36c144116beae9a06566570e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Self-Score: 0.44\n",
      "results for chunk size 2048\n",
      "Rouge Metrics: {'rouge1': 0.20118067932580136, 'rouge2': 0.06155318333280251, 'rougeL': 0.1614104484819238, 'rougeLsum': 0.16605004609587115}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5bd8663ce247c6bfe7d9a9a9e54429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Self-Score: 0.39\n"
     ]
    }
   ],
   "source": [
    "for idx in range(6):\n",
    "    output_file = f\"output/question_generation_with_corresponding_in_context_mistral_large_chunksize{chunk_sizes[idx]}.jsonl\"\n",
    "    rouge_metrics = get_rouge_metrics(output_file)\n",
    "    print(\"results for chunk size\", chunk_sizes[idx])\n",
    "    print(\"Rouge Metrics:\", rouge_metrics)\n",
    "\n",
    "    self_score = llm_self_score(output_file, llm=gpt4)\n",
    "    print(\"LLM Self-Score:\", self_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LongRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

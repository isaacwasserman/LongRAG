{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Abhishek Goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "import shutil\n",
    "import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0.2, model=\"gpt-4\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Split the document text into chunks based on a fixed character limit.\n",
    "\n",
    "    Args:\n",
    "        text (str): The entire document text.\n",
    "        chunk_size (int): The maximum number of characters in each chunk.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        if current_length + len(word) > chunk_size and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word)\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += len(word) + 1  # Adding 1 for the space.\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunks(chunks, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Generate a summary for each text chunk using GPT-4.\n",
    "\n",
    "    Args:\n",
    "        chunks (list[str]): A list of text chunks to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of summaries for each chunk.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        prompt = f\"Summarize the following text conceptually:\\n\\n{chunk}\"\n",
    "        response = Settings.llm.complete(prompt).text\n",
    "        summaries.append(response.strip())\n",
    "\n",
    "    #open summary save file name having chunk size in name\n",
    "    with open(f\"summaries_file_{chunk_size}.txt\", 'w', encoding='utf-8') as file:\n",
    "        for summary in summaries:\n",
    "            file.write(summary + '\\n')  # Each summary on a new line\n",
    "\n",
    "    return summaries\n",
    "\n",
    "def load_summaries(summaries_file='summaries_file_500.txt'):\n",
    "    \"\"\"\n",
    "    Load summaries from a file where each summary is saved on a new line.\n",
    "\n",
    "    Args:\n",
    "        summaries_file (str): The file path from which to load summaries.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of summaries loaded from the file.\n",
    "    \"\"\"\n",
    "    with open(summaries_file, 'r', encoding='utf-8') as file:\n",
    "        summaries = [line.strip() for line in file if line.strip()]  # Exclude empty lines\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_from_text_with_ids(text, title, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Create an index from context, assigning a unique ID to each text chunk without including the ID in the text used for embedding.\n",
    "\n",
    "    Args:\n",
    "        text (str): The context to index.\n",
    "        title (str): The title of the index for access later.\n",
    "        chunk_size (int): The maximum number of characters in each chunk.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The index created from the context, with IDs assigned to each text chunk.\n",
    "    \"\"\"\n",
    "    chunks = chunk_text(text, chunk_size=chunk_size)\n",
    "    index = get_index_by_title(title)\n",
    "\n",
    "    if index is None:\n",
    "        index = VectorStoreIndex()\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Generate embedding for the chunk without the ID in the text\n",
    "            embedding = Settings.embed_model.embed(chunk)\n",
    "            \n",
    "            # Assign a unique ID to the chunk's embedding\n",
    "            chunk_id = f\"text_chunk_{i}\"\n",
    "            \n",
    "            # Add the embedding to the index with the unique ID, not including the ID in the text\n",
    "            index.add_document(chunk_id, embedding)\n",
    "\n",
    "        index.set_index_id(title)\n",
    "        index.storage_context.persist()\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "def add_summarization_embeddings_with_ids(index, chunk_size=500, new_storage_location=\"storage_same_chunk_summary\", index_title=\"summary_index\"):\n",
    "    \"\"\"\n",
    "    Generate embeddings for text summaries and add them to the existing vector store, assigning unique IDs to summaries without including the ID in the text for embedding.\n",
    "\n",
    "    Args:\n",
    "        index (VectorStoreIndex): The existing vector store index.\n",
    "        summaries (list[str]): A list of text summaries to be embedded and added.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The updated index with added summary embeddings and their unique IDs.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    chunks = chunk_text(summaries, chunk_size=chunk_size)\n",
    "    summaries = summarize_chunks(chunks, chunk_size=chunk_size)\n",
    "    \n",
    "    base_id = len(index)  # Starting point for new IDs\n",
    "    title = index.index_id + index_title\n",
    "    index_temp = get_index_by_title(title)\n",
    "    if index_temp is not None:\n",
    "        return index_temp\n",
    "    \n",
    "    for i, summary in enumerate(summaries):\n",
    "        # Generate embedding for the summary without the ID in the text\n",
    "        embedding = Settings.embed_model.embed(summary)\n",
    "        \n",
    "        # Assign a unique ID to the summary's embedding\n",
    "        summary_id = f\"summary_{base_id + i}\"\n",
    "        \n",
    "        # Add the embedding to the index with the unique ID, not including the ID in the text\n",
    "        index.add_document(summary_id, embedding)\n",
    "    \n",
    "    index.set_index_id(title)\n",
    "    new_storage_context = StorageContext.from_defaults(persist_dir=new_storage_location)\n",
    "    index.storage_context = new_storage_context  # Update the index's storage context to the new location\n",
    "    index.storage_context.persist()  # Persist the index in the new location\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_from_text(text, title):\n",
    "    \"\"\"\n",
    "    Create an index from a piece of context. If an index with the given title already exists, it will be returned.\n",
    "\n",
    "    Args:\n",
    "        text (str): The context to index\n",
    "        title (str): The title of the index for access later\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The index created from the context\n",
    "    \"\"\"\n",
    "    index = get_index_by_title(title)\n",
    "    if index is None:\n",
    "        os.makedirs(\"tmp\", exist_ok=True)\n",
    "        with open(\"tmp/tmp.txt\", \"w\") as f:\n",
    "            f.write(text)\n",
    "        documents = SimpleDirectoryReader(\"tmp\").load_data()\n",
    "        index = VectorStoreIndex.from_documents(documents, model_name=\"openai/text-embedding-3-small\")\n",
    "        index.set_index_id(title)\n",
    "        index.storage_context.persist()\n",
    "        shutil.rmtree(\"tmp\")\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "def get_index_by_title(title):\n",
    "    \"\"\"\n",
    "    Get an index by its title. If the index does not exist, returns None.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the index\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The index with the specified title\n",
    "    \"\"\"\n",
    "    try:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "        index = load_index_from_storage(storage_context, index_id=title)\n",
    "        return index\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_reading_comprehension(question, context_title=None, context=\"\", use_rag=False):\n",
    "    \"\"\"\n",
    "    Answer a question given a context. If use_rag is True, retrieval will be used to answer the question. Otherwise, the entire context will be prepended to the question.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_title (str): The title of the context index\n",
    "        context (str): The context to use\n",
    "        use_rag (bool): Whether to use retrieval to answer the question\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "    if use_rag:\n",
    "        if context_title is None:\n",
    "            raise ValueError(\"context_title must be provided when using RAG\")\n",
    "        #index = create_index_from_text(context, context_title)\n",
    "        index = create_index_from_text_with_ids(context, context_title, chunk_size=500)\n",
    "\n",
    "        index = add_summarization_embeddings_with_ids(index, chunk_size=500, new_storage_location=\"storage_same_chunk_summary\", index_title=\"summary_index\")\n",
    "        \n",
    "        query_engine = index.as_query_engine()\n",
    "        # Query the vector store to find top matching chunks\n",
    "        top_chunks = query_engine.query(question, top_k=top_k)\n",
    "        \n",
    "        # Extract and combine the text from the top matching chunks for use as context\n",
    "        top_chunks_text = ' '.join([chunk['text'] for chunk in top_chunks])\n",
    "\n",
    "        raw_text_chunk_count =0\n",
    "        summary_chunk_count = 0\n",
    "        for chunk in top_chunks:\n",
    "            if chunk['text'].startswith(\"text_chunk_\"):\n",
    "                raw_text_chunk_count += 1\n",
    "            elif chunk['text'].startswith(\"summary_\"):\n",
    "                summary_chunk_count += 1\n",
    "\n",
    "        print(f\"Raw text chunk count: {raw_text_chunk_count}\")\n",
    "        print(f\"Summary chunk count: {summary_chunk_count}\")\n",
    "        \n",
    "        # Use the combined context of top chunks to generate an answer to the question\n",
    "        # Construct a prompt that guides the LLM to consider the context and answer the question subjectively or conceptually\n",
    "        prompt = f\"Consider the following context with depth and thoughtfulness: {top_chunks_text}\\n\\nCan you respond to the following question with insight and nuance:?\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        #combined_context = top_chunks_text + \"\\n\" + question\n",
    "        response = Settings.llm.complete(prompt).text\n",
    "\n",
    "        # response = query_engine.query(question).response\n",
    "    else:\n",
    "        response = Settings.llm.complete(context + \"\\n\" + question).text\n",
    "    return response\n",
    "\n",
    "\n",
    "def answer_reading_comprehension_with_rag(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Answer a question given a context using retrieval to answer the question.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_title (str): The title of the context index\n",
    "        context (str): The context to use\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "    return answer_reading_comprehension(*args, **kwargs, use_rag=True)\n",
    "\n",
    "\n",
    "def answer_reading_comprehension_in_context(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Answer a question given a context. The context will be prepended to the question.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context (str): The context to use\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "    return answer_reading_comprehension(*args, **kwargs, use_rag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/miniforge3/envs/LongRAG/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for bigainlco/LooGLE contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigainlco/LooGLE\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "longdep_qa_ds = load_dataset(\"bigainlco/LooGLE\", \"longdep_qa\", split=\"test\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def get_rouge_metrics(output_file):\n",
    "    \"\"\"\n",
    "    Get ROUGE metrics for a .jsonl file containing generated answers and ground truth answers.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): The path to the .jsonl file\n",
    "\n",
    "    Returns:\n",
    "        dict: The ROUGE metrics\n",
    "    \"\"\"\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    outputs = [json.loads(line) for line in lines]\n",
    "    generated_answers = [output[\"generated_answer\"] for output in outputs]\n",
    "    ground_truths = [output[\"ground_truth\"] for output in outputs]\n",
    "    rouge_metrics = rouge.compute(predictions=generated_answers, references=ground_truths)\n",
    "    return rouge_metrics\n",
    "\n",
    "\n",
    "def llm_self_score(output_file):\n",
    "    \"\"\"\n",
    "    Score the generated answers in a .jsonl file using the LLM. The user will be prompted to determine whether each answer is correct.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): The path to the .jsonl file\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the generated answers\n",
    "    \"\"\"\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    outputs = [json.loads(line) for line in lines]\n",
    "    llm = Settings.llm\n",
    "    for output in outputs:\n",
    "        question = output[\"question\"]\n",
    "        ground_truth = output[\"ground_truth\"]\n",
    "        generated_answer = output[\"generated_answer\"]\n",
    "        if \"correct\" in output:\n",
    "            continue\n",
    "        prompt = f'Given the question \"{question}\" whose answer is \"{ground_truth}\", is answer \"{generated_answer}\" similar enough to the true answer that it should be considered correct? Answer \"yes\" or \"no\" with no other characters or capitalization.'\n",
    "        response = llm.complete(prompt).text.lower()\n",
    "        if \"yes\" in response:\n",
    "            output[\"correct\"] = True\n",
    "        else:\n",
    "            output[\"correct\"] = False\n",
    "    num_correct = sum([output[\"correct\"] for output in outputs])\n",
    "    accuracy = num_correct / len(outputs)\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for output in outputs:\n",
    "            json.dump(output, f)\n",
    "            f.write(\"\\n\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference function and helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_output_file(output_file):\n",
    "    \"\"\"\n",
    "    Read a .jsonl file containing generated answers and ground truth answers.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): The path to the .jsonl file\n",
    "\n",
    "    Returns:\n",
    "        list: The outputs in the file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_file):\n",
    "        return []\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    outputs = [json.loads(line) for line in lines]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def log_outputs(question, ground_truth, generated_answer, output_file):\n",
    "    \"\"\"\n",
    "    Log a question, its ground truth answer, and a generated answer to a .jsonl file.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question\n",
    "        ground_truth (str): The ground truth answer\n",
    "        generated_answer (str): The generated answer\n",
    "        output_file (str): The path to the .jsonl file; if None, a new file will be created in the \"output\" directory with the current time as the name\n",
    "\n",
    "    Returns:\n",
    "        tuple: The path to the .jsonl file and the outputs in the file\n",
    "    \"\"\"\n",
    "    if output_file is None:\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        output_file = f\"output/{time.time()}.jsonl\"\n",
    "    with open(output_file, \"a\") as f:\n",
    "        json.dump({\"question\": question, \"ground_truth\": ground_truth, \"generated_answer\": generated_answer}, f)\n",
    "        f.write(\"\\n\")\n",
    "    existing_output = read_output_file(output_file)\n",
    "    return output_file, existing_output\n",
    "\n",
    "\n",
    "def question_is_answered(question, existing_output):\n",
    "    \"\"\"\n",
    "    Determine whether a question has already been answered in a list of outputs.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question\n",
    "        existing_output (list): The outputs\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the question has already been answered\n",
    "    \"\"\"\n",
    "    if question in [output[\"question\"] for output in existing_output]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def test_longdep_qa(inference_function, output_file=None, debug_lim=None):\n",
    "    \"\"\"\n",
    "    Test an inference function on the longdep_qa dataset.\n",
    "\n",
    "    Args:\n",
    "        inference_function (function): The function to test\n",
    "        output_file (str): The path to the .jsonl file to log outputs to; if None, a new file will be created in the \"output\" directory with the current time as the name\n",
    "        debug_lim (int): The number of questions to test; if None, all questions will be tested\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    n_questions = sum([len(eval(env[\"qa_pairs\"])) for env in longdep_qa_ds])\n",
    "    if debug_lim is None:\n",
    "        debug_lim = n_questions\n",
    "    existing_output = read_output_file(output_file)\n",
    "    with tqdm.tqdm(total=debug_lim) as pbar:\n",
    "        for environment in longdep_qa_ds:\n",
    "            context = environment[\"input\"]\n",
    "            title = environment[\"title\"]\n",
    "            qa_pairs = eval(environment[\"qa_pairs\"])\n",
    "            for question_dict in qa_pairs:\n",
    "                question = question_dict[\"Q\"]\n",
    "                ground_truth = question_dict[\"A\"]\n",
    "                if not question_is_answered(question, existing_output):\n",
    "                    generated_answer = inference_function(question, context_title=title, context=context)\n",
    "                    output_file, existing_output = log_outputs(question, ground_truth, generated_answer, output_file)\n",
    "                pbar.update(1)\n",
    "                if pbar.n >= debug_lim:\n",
    "                    break\n",
    "            if pbar.n >= debug_lim:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 22030.06it/s]\n"
     ]
    }
   ],
   "source": [
    "test_longdep_qa(answer_reading_comprehension_with_rag, output_file=\"output/baseline_with_rag_100.jsonl\", debug_lim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge Metrics: {'rouge1': 0.19845643848785122, 'rouge2': 0.07862381462209828, 'rougeL': 0.1619569034833745, 'rougeLsum': 0.17183036257033626}\n",
      "LLM Self-Score: 0.53\n"
     ]
    }
   ],
   "source": [
    "rouge_metrics = get_rouge_metrics(\"output/baseline_with_rag_100.jsonl\")\n",
    "print(\"Rouge Metrics:\", rouge_metrics)\n",
    "\n",
    "llm_self_score = llm_self_score(\"output/baseline_with_rag_100.jsonl\")\n",
    "print(\"LLM Self-Score:\", llm_self_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LongRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

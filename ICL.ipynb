{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from rag_components import *\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks, summarizer_llm=mixtral, text_title=None):\n",
    "    \"\"\"\n",
    "    Generate a summary for each text chunk using the summarizer_llm.\n",
    "\n",
    "    Args:\n",
    "        chunks (list[str]): A list of text chunks to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Summaries of the text chunks.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    if text_title is None:\n",
    "        text_title = \"chunks\"\n",
    "    for chunk in tqdm(chunks, desc=f'Summarizing \"{text_title}\"', leave=False):\n",
    "        if len(summaries) == 0:\n",
    "            prompt = f\"\"\"<s>[INST]Summarize the following text conceptually. \\\n",
    "                The summary should paraphrase the original text, be significantly \\\n",
    "                shorter, retain all propositions, and be able to replace the \\\n",
    "                original text. Here is the text to be summarized:\\n\\n{chunk}[/INST]\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"<s>[INST]Summarize the following text conceptually in the context of the text that precedes it. \\\n",
    "                The summary should paraphrase the original text, be significantly \\\n",
    "                shorter, retain all propositions, and be able to replace the \\\n",
    "                original text. Here is the the context:\\n\\n{summaries[-1]}\\n\\n\\\n",
    "                Summarize the following text:\\n\\n{chunk}[/INST]\"\"\"\n",
    "\n",
    "        prompt = re.sub(r\"\\s+\", \" \", prompt)\n",
    "        response = summarizer_llm.complete(prompt).text.strip()\n",
    "        summaries.append(response)\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def summarize_index(text_title, overwrite_existing=False, summarizer_llm=Settings.llm):\n",
    "    \"\"\"\n",
    "    Generate a summary for each text chunk in the index using the summarizer_llm.\n",
    "\n",
    "    Args:\n",
    "        index (VectorStoreIndex): The index containing text chunks to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: An index containing summaries for each text chunk.\n",
    "    \"\"\"\n",
    "    if not overwrite_existing:\n",
    "        existing_index = get_index_by_title(f\"{text_title}_summaries\")\n",
    "        if existing_index:\n",
    "            return existing_index\n",
    "    index = get_index_by_title(text_title)\n",
    "    text_chunk_ids = get_ids_from_index(index)\n",
    "    id_nums = [int(id_.split(\"_\")[-1]) for id_ in text_chunk_ids]\n",
    "    summary_ids = [f\"summary_{id_num}\" for id_num in id_nums]\n",
    "    chunks = [get_text_by_id(index, node_id) for node_id in text_chunk_ids]\n",
    "    summaries = summarize_chunks(chunks, summarizer_llm, text_title=text_title)\n",
    "\n",
    "    summary_index = create_index_from_chunks_with_ids(\n",
    "        summaries, summary_ids, f\"{text_title}_summaries\", overwrite_existing=overwrite_existing\n",
    "    )\n",
    "    return summary_index\n",
    "\n",
    "def answer_reading_comprehension_icl(question, retrieved_chunks_combined, qa_llm=mistral_large, examplelist=None):\n",
    "    if len(examplelist) > 0:\n",
    "        prompt = generate_qa_prompt_icl(retrieved_chunks_combined, question, examplelist)\n",
    "    else:\n",
    "        prompt = generate_qa_prompt(retrieved_chunks_combined, question)\n",
    "    response = qa_llm.complete(prompt).text\n",
    "    return response\n",
    "\n",
    "def answer_reading_comprehension_with_summarization_icl(\n",
    "    question, context_title, context_text, top_k=2, chunk_size=1024, chunk_overlap=200, qa_llm=gpt4, examplelist=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question given a context.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_title (str): The title of the context\n",
    "        context_text (str): The text of the context\n",
    "        top_k (int): The number of top matching chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the top k most relevant summary chunks\n",
    "    text_index_title = f\"{context_title}\"\n",
    "    summary_index = summarize_index(text_index_title, summarizer_llm=mixtral)\n",
    "    summary_retriever = VectorIndexRetriever(\n",
    "        index=summary_index,\n",
    "        similarity_top_k=top_k,\n",
    "    )\n",
    "    top_summary_chunks = summary_retriever.retrieve(question)\n",
    "\n",
    "    # Identify the ids of the corresponding raw text chunks\n",
    "    retrieved_summary_ids = [chunk.node.id_ for chunk in top_summary_chunks]\n",
    "    corresponding_text_ids = [f\"text_chunk_{summary_id.split('_')[-1]}\" for summary_id in retrieved_summary_ids]\n",
    "\n",
    "    # Get the correponding raw text chunks\n",
    "    text_index = create_index_from_text_with_ids(\n",
    "        context_text, text_index_title, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    corresonding_chunks = [get_node_by_id(text_index, text_id) for text_id in corresponding_text_ids]\n",
    "\n",
    "    # Find the top k * 10 most relevant raw text chunks, and try to find the rank of the chunks which correspond to the top summary chunks\n",
    "    text_retriever = VectorIndexRetriever(\n",
    "        index=text_index,\n",
    "        similarity_top_k=top_k * 10,\n",
    "    )\n",
    "    retrieved_texts = text_retriever.retrieve(question)\n",
    "    text_chunk_ranks = []\n",
    "    for i, chunk in enumerate(corresonding_chunks):\n",
    "        id_ = chunk.id_\n",
    "        rank = None\n",
    "        for j, retrieved_text in enumerate(retrieved_texts):\n",
    "            if retrieved_text.node.id_ == id_:\n",
    "                rank = j\n",
    "                break\n",
    "        text_chunk_ranks.append(rank)\n",
    "\n",
    "    corresponding_chunks_text = [chunk.text for chunk in corresonding_chunks]\n",
    "    corresponding_chunks_text_combined = \" \".join(corresponding_chunks_text)\n",
    "\n",
    "    response = answer_reading_comprehension_icl(question, corresponding_chunks_text_combined, qa_llm=qa_llm, examplelist=examplelist)\n",
    "\n",
    "    top_chunks_info = [\n",
    "        {\n",
    "            \"summary_score\": top_summary_chunks[i].score,\n",
    "            \"summary_rank\": i,\n",
    "            \"text_score\": similarity_score(question, text_index_title, corresonding_chunks[i].id_),\n",
    "            \"text_rank\": text_chunk_ranks[i],\n",
    "            \"summary\": top_summary_chunks[i].node.text,\n",
    "            \"text\": corresonding_chunks[i].text,\n",
    "        }\n",
    "        for i in range(len(corresonding_chunks))\n",
    "    ]\n",
    "\n",
    "    additional_info = {\n",
    "        \"top_chunks_info\": top_chunks_info,\n",
    "    }\n",
    "    return response, additional_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_lim = 205\n",
    "chunk_sizes = [256]\n",
    "topks = [8]\n",
    "chunk_overlaps = [50]\n",
    "for idx in range(1):\n",
    "    print(\"Generating output for chunk size\", chunk_sizes[idx])\n",
    "    output_file = f\"output/summarization_with_corresponding_in_context_mistral_large_icl_chunksize{chunk_sizes[idx]}.jsonl\"\n",
    "    test_longdep_qa(\n",
    "        answer_reading_comprehension_with_summarization_icl,\n",
    "        output_file=output_file,\n",
    "        debug_lim=debug_lim,\n",
    "        qa_llm=mistral_large,\n",
    "        chunk_size=chunk_sizes[idx],\n",
    "        top_k=topks[idx],\n",
    "        chunk_overlap=chunk_overlaps[idx],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
